#+title: Personal Website Content
#+author: David Álvarez Rosa
#+startup: logdone
#+filetags: :pers:blog:
#+CATEGORY: Blog

* TODO [#B] How to setup a tunnel from vps to local homelab         :backlog:
:PROPERTIES:
:EXPORT_FILE_NAME: pending-tunnel
:END:

Write a blog on how to setup a tunnel from vps to local homelab

* TODO [#B] Incremental clang-tidy tool                             :backlog:
:PROPERTIES:
:EXPORT_FILE_NAME: pending-incremental
:END:

How to incrementally run clang-tidy or cppcheck

* TODO [#B] Spin-Lock                                               :backlog:
:PROPERTIES:
:EXPORT_FILE_NAME: spin-lock
:END:

Pending. Optimizing a spin-lock using this youtube series.
Might be wrong with the right youtube series bla bla bla more text

https://www.youtube.com/watch?v=AN6XHy2znzc

Pending.

* TODO [#B] SFINAE                                                  :backlog:

Pending.

* TODO [#B] Lru Cache                                               :backlog:

Pending.

* TODO [#B] Small buffer optimization                               :backlog:

Specially small string optimization.

* TODO [#B] Optimizing Matrix Multiplication                        :backlog:

Fastest matrix multiplication.

* TODO [#B] Translation Look Aside Buffer                           :backlog:
:PROPERTIES:
:EXPORT_FILE_NAME: translation-look-aside-buffer
:END:

Explain what the TLB is, using maybe hrt blog?

* TODO [#B] Vector push_back                                        :backlog:

In-depth vector push_back following guide

* TODO [#B] Implementing a Shared_ptr                               :backlog:

Implement a shared_ptr

* TODO [#B] Exploring CPU Caches
:PROPERTIES:
:EXPORT_FILE_NAME: exploring-cpu-caches
:END:

Pending.

* DONE [#B] Optimizing a Lock-Free Ring Buffer
CLOSED: [2026-01-19 Sat 13:32]
:PROPERTIES:
:EXPORT_FILE_NAME: optimizing-a-lock-free-ring-buffer
:END:
:LOGBOOK:
CLOCK: [2026-01-24 Sat 10:47]--[2026-01-24 Sat 11:35] =>  0:48
CLOCK: [2026-01-12 Mon 19:13]--[2026-01-21 Wed 19:32] => 216:19
:END:

A single-producer single-consumer (SPSC) queue is a great example of how
far constraints can take a design.  In this post, you will learn how to
implement a ring buffer from scratch: start with the simplest design,
make it thread-safe, and then gradually remove overhead while preserving
FIFO behavior and predictable latency.  This pattern is widely used to
share data between threads in the lowest-latency environments.

** What is a ring buffer?

 [fn:11]You might have run into the term circular buffer, or perhaps
cyclic queue.  These are simply other names for a /ring buffer:/ a queue
where a producer generates data and inserts it into the buffer, and a
consumer later pulls it back out, in first-in-first-out order.

What makes a ring buffer distinctive is how it stores data and the
constraints it enforces.  It has a fixed capacity; it neither expands
nor shrinks.  As a result, when the buffer fills up, the producer must
either wait until space becomes available or overwrite entries that have
not been read yet, depending on what the application expects.

The consumer's job is straightforward: read items as they arrive.  When
the ring buffer is empty, the consumer has to block, spin, or move on to
other work.  Each successful read releases a slot the producer can
reuse.  In the ideal case, the producer stays just a bit ahead, and the
system turns into a quiet game of /"catch me if you can,"/ with minimal
waiting on both sides.

** Single-threaded ring buffer

Let's start with a single-threaded ring buffer, which is just an
array[fn:1] and two indices.  We leave one slot permanently unused to
distinguish "full" from "empty."  Push writes to head and advances it;
pop reads from tail and advances it.

#+begin_src cpp
  template <typename T, std::size_t N>
  class RingBufferV1 {
    std::array<T, N> buffer_;
    std::size_t head_{0};
    std::size_t tail_{0};
  };
#+end_src

We can now implement the ~push~ (or write) operation[fn:2]

#+begin_src cpp
  auto push(const T& value) noexcept -> bool {
    auto new_head = head_ + 1;
    if (new_head == buffer_.size()) [[unlikely]] {  // Wrap-around
      new_head = 0;
    }
    if (new_head == tail_) [[unlikely]] {  // Full
      return false;
    }
    buffer_[head_] = value;
    head_ = new_head;
    return true;
  }
#+end_src

Next we implement the ~pop~ (or read) operation[fn:3]

#+begin_src cpp
  auto pop(T& value) noexcept -> bool {
    if (head_ == tail_) [[unlikely]] {  // Empty
      return false;
    }
    value = buffer_[tail_];
    auto next_tail = tail_ + 1;
    if (next_tail == buffer_.size()) [[unlikely]] {  // Wrap-around
      next_tail = 0;
    }
    tail_ = next_tail;
    return true;
  }
#+end_src

** Thread-safe ring buffer

You probably already noticed that the previous version is not
thread-safe.  The easiest way to solve this is to add a ~mutex~ around
push and pop.

#+begin_src cpp
  template <typename T, std::size_t N>
  class RingBufferV2 {
    std::mutex mutex_;

    auto push(const T& value) noexcept -> bool {
      auto lock = std::lock_guard<std::mutex>{mutex_};  // Thread-safe
      // ...
    }

    auto pop(T& value) noexcept -> bool {
      auto lock = std::lock_guard<std::mutex>{mutex_};  // Thread-safe
      // ...
    }
  };
#+end_src

It's correct and often fast enough: around *12M ops/s*[fn:14] on
consoomer hardware.  However, it pays for mutual exclusion even though
the producer and consumer never write the same index.  The ownership is
asymmetric: the producer is the only writer of head, and the consumer is
the only writer of tail.  That asymmetry is the lever to remove locks.

** Lock-free ring buffer

We can remove the locks by using atomics instead[fn:15]

#+begin_src cpp
  template <typename T, std::size_t N>
  class RingBufferV3 {
    std::array<T, N> buffer_;
    alignas(std::hardware_destructive_interference_size) std::atomic_size_t head_{0};
    alignas(std::hardware_destructive_interference_size) std::atomic_size_t tail_{0};
  };
#+end_src

The push implementation becomes

#+begin_src cpp
  auto push(const T& value) noexcept -> bool {
    const auto head = head_.load();
    auto next_head = head + 1;
    if (next_head == buffer_.size()) [[unlikely]] {
      next_head = 0;
    }
    if (next_head == tail_.load()) [[unlikely]] {
      return false;
    }
    buffer_[head] = value;
    head_.store(next_head);
    return true;
  }
#+end_src

And the pop implementation

#+begin_src cpp
  auto pop(T& value) noexcept -> bool {
    const auto tail = tail_.load();
    if (tail == head_.load()) [[unlikely]] {
      return false;
    }
    value = buffer_[tail];
    auto next_tail = tail + 1;
    if (next_tail == buffer_.size()) [[unlikely]] {
      next_tail = 0;
    }
    tail_.store(next_tail);
    return true;
  }
#+end_src

Simply removing the locks yields *35M ops/s*, more than double the
throughput of the locked version!  You have probably noticed that we are
using the default ~std::memory_order_seq_cst~ memory order for loading /
storing the atomics, which is the slowest.  Let's manually tune the
memory order[fn:25]

#+begin_src cpp
  auto push(const T& value) noexcept -> bool {
    const auto head = head_.load(std::memory_order_relaxed);
    auto next_head = head + 1;
    if (next_head == buffer_.size()) [[unlikely]] {
      next_head = 0;
    }
    if (next_head == tail_.load(std::memory_order_acquire)) [[unlikely]] {
      return false;
    }
    buffer_[head] = value;
    head_.store(next_head, std::memory_order_release);
    return true;
  }

  auto pop(T& value) noexcept -> bool {
    const auto tail = tail_.load(std::memory_order_relaxed);
    if (tail == head_.load(std::memory_order_acquire)) [[unlikely]] {
      return false;
    }
    value = buffer_[tail];
    auto next_tail = tail + 1;
    if (next_tail == buffer_.size()) [[unlikely]] {
      next_tail = 0;
    }
    tail_.store(next_tail, std::memory_order_release);
    return true;
  }
#+end_src

Rerunning the benchmark now gives an astonishing *108M ops/s*---3x the
previous version and 9x the original locked version.  Worth the effort,
right?

** Further optimization

We already have a fast ring buffer, but we can push it further.  The
main slowdown comes from the reader and writer constantly touching each
other's indexes.  That makes the CPU bounce cache lines[fn:17] between cores,
which is expensive.

To reduce this, the reader can keep a local cached copy[fn:16] of the write
index, and the writer keeps a local cached copy of the read index.  Then
they don't need to re-check the other side on every single operation:
only once in a while.

#+begin_src cpp
  template <typename T, std::size_t N>
  class RingBufferV5 {
    std::array<T, N> buffer_;
    alignas(std::hardware_destructive_interference_size) std::atomic_size_t head_{0};
    alignas(std::hardware_destructive_interference_size) std::size_t head_cached_{0};
    alignas(std::hardware_destructive_interference_size) std::atomic_size_t tail_{0};
    alignas(std::hardware_destructive_interference_size) std::size_t tail_cached_{0};
  };
#+end_src

The push operation is updated to first consult the cached tail
~tail_cached_~ and if that fails retry after updating the cache

#+begin_src cpp
  if (next_head == tail_cached_) [[unlikely]] {
    tail_cached_ = tail_.load(std::memory_order_acquire);
    if (next_head == tail_cached_) {
      return false;
    }
  }
#+end_src

The pop operation is updated in a similar way to first consult the
cached head

#+begin_src cpp
  if (tail == head_cached_) [[unlikely]] {
    head_cached_ = head_.load(std::memory_order_acquire);
    if (tail == head_cached_) {
      return false;
    }
  }
#+end_src

Throughput is now *305M ops/s*---nearly 3x faster than the manually
tuned lock-free version and 25x faster than the original locking
approach.

** Summary

If you want to reproduce these results, run the included [[https://github.com/david-alvarez-rosa/CppPlayground/blob/main/DataStructures/ring_buffer.cpp][benchmark]]
compiled with at least ~-O3~ optimization level.[fn:21] The benchmark
pins the producer and consumer threads to dedicated CPU cores to
minimize scheduling noise.

| Version | Throughput | Notes                                            |
|---------+------------+--------------------------------------------------|
|       1 | N/A        | Not thread-safe                                  |
|       2 | 12M ops/s  | Mutex / lock                                     |
|       3 | 35M ops/s  | Lock-free (atomics)                              |
|       4 | 108M ops/s | Lock-free (atomics) + memory order               |
|       5 | 305M ops/s | Lock-free (atomics) + memory order + index cache |

Long live lock-free and wait-free data structures!

* DONE [#B] Deriving Type Erasure
CLOSED: [2026-01-24 Sat 20:06]
:PROPERTIES:
:EXPORT_FILE_NAME: deriving-type-erasure
:END:
:LOGBOOK:
CLOCK: [2026-01-24 Sat 18:10]--[2026-01-24 Sat 20:06] =>  1:56
CLOCK: [2026-01-24 Sat 10:47]--[2026-01-24 Sat 10:47] =>  0:00
:END:

Ever looked at ~std::any~ and wondered what's actually going on behind
the scenes?  Beneath the intimidating interface is a clean case of type
erasure: concrete types hidden behind a small, uniform wrapper.

Starting from familiar tools---virtual functions and templates---we'll
build a minimal ~std::any~.  Along the way, type erasure shifts from
buzzword to practical technique you can recognize and reuse in your own
designs.

** Polymorphism with interfaces

The typical way to achieve polymorphism is to define an interface
consisting of pure-virtual methods you want to be able to call.  Then,
for each implementation that you want to use polymorphically, you create
a subclass that inherits from the base class and implement those
methods.

As an example, let's implement shape classes that have an ~area()~
method.  We start with an interface[fn:10] class

#+begin_src cpp
  class Shape {
  public:
    virtual ~Shape() = default;
    virtual auto area() const noexcept -> double = 0;
  };
#+end_src

And add a couple of concrete implementations for ~Square~ and ~Circle~

#+begin_src cpp
  class Square : public Shape {
    int side_;
  public:
    explicit Square(int side) noexcept : side_{side} {}
    auto area() const noexcept -> double override { return side_ * side_; }
  };

  class Circle : public Shape {
    int radius_;
  public:
    explicit Circle(int radius) noexcept : radius_{radius} {}
    auto area() const noexcept -> double override {
      return std::numbers::pi * radius_ * radius_;
    }
  };
#+end_src

Now, we can use these implementations generically, by coding against the
interface

#+begin_src cpp
  auto printArea(const Shape& shape) -> void {
    std::println("Area is {:.2f}", shape.area());
  }
#+end_src

Simple enough, right?

** Polymorphism with templates

Inheritance is a good solution to problems that require polymorphism,
but sometimes the concrete types you want to handle polymorphically
cannot share a common base class.[fn:12] In that case, if the types
provide the same interface, you can use a template to get polymorphism
instead

#+begin_src cpp
  auto printArea(const auto& shape) -> void {
    std::println("Area is {:.2f}", shape.area());
  }
#+end_src

You can use this method with ~Square~, ~Circle~, or any type that
provides a zero-argument ~area()~ returning ~double~. Templates work
because the compiler generates a version of the function for each
concrete type you use, and the call is valid as long as that generated
code would compile[fn:13] for the given type.

Unfortunately, template-based polymorphism has two main downsides.

*First,* templates do not give you one shared runtime base type like
~Shape~. Each instantiation is a distinct type, so there is no common
type for a homogeneous container; you cannot store a mix of ~Square~ and
~Circle~ in one array and handle them uniformly the way you can with a
pointer to base technique

#+begin_src cpp
  auto shapes = std::vector< ??? >{&square, &circle};
#+end_src

The *second* drawback[fn:20] is a little more subtle.  Anybody who uses
the template-based ~area(const auto&)~ method must either explicitly
specify the concrete type, or be a template itself, to pass along the
template type of ~area()~.

** Deriving std::any

Imagine ~Square~ and ~Circle~ are fixed types with no shared base class,
and you cannot change them to inherit from one.  But you still want to
handle them through a single common interface.

One way to do that is to introduce wrappers.  Define your own ~Shape~
interface, then create wrapper classes that inherit from ~Shape~ and
contain a ~Square~ or ~Circle~; each wrapper implements the virtual
methods by simply forwarding calls to the wrapped object

#+begin_src cpp
  class SquareWrapper : public Shape {
    Square square_;
  public:
    explicit SquareWrapper(Square square) noexcept : square_{std::move(square)} {}
    auto area() const noexcept -> double override { return square_.area(); }
  };

  class CircleWrapper : public Shape {
    Circle circle_;
  public:
    explicit CircleWrapper(Circle circle) noexcept : circle_{std::move(circle)} {}
    auto area() const noexcept -> double override { return circle_.area(); }
  };
#+end_src

Now we can work directly with instances of ~Shape~

#+begin_src cpp
  auto printAreas(const std::vector<Shape*>& shapes) -> void {
    for (auto* shape : shapes) {
      std::println("Area is {:.2f}", shape->area());
    }
  }

  auto main() -> int {
    auto square = SquareWrapper{Square{2}};
    auto circle = CircleWrapper{Circle{1}};
    auto shapes = std::vector<Shape*>{&square, &circle};
    printAreas(shapes);
  }
#+end_src

This approach works, but it has an obvious downside: you need a separate
wrapper type (like ~CircleWrapper~) for every concrete type you want to
adapt (like ~Circle~), which quickly turns into a pile of
boilerplate. Luckily, templates can offload much of that work to the
compiler by generating the needed code for each type automatically

#+begin_src cpp
  template <typename T>
  class ShapeWrapper : public Shape {
    T shape_;
  public:
    explicit ShapeWrapper(T shape) noexcept : shape_{std::move(shape)} {}
    auto area() const noexcept -> double override { return shape_.area(); }
  };
#+end_src

What we built above is the basis of the "type erasure" idiom.  All
that's left is to hide all this machinery behind another class, so that
callers don't have to deal with our custom interfaces and
templates[fn:18]

#+begin_src cpp
  class AnyShape {
    class Shape {  // The interface
    public:
      virtual ~Shape() = default;
      virtual auto area() const noexcept -> double = 0;
    };

    template <typename T>
    class ShapeWrapper : public Shape {  // The wrappers
      T shape_;

    public:
      explicit ShapeWrapper(T shape) noexcept : shape_{std::move(shape)} {}
      auto area() const noexcept -> double override { return shape_.area(); }
    };

    std::unique_ptr<Shape> shape_;

  public:
    template <typename T>
    explicit AnyShape(T&& shape)
        : shape_{std::make_unique<ShapeWrapper<T>>(std::forward<T>(shape))} {}

    auto area() const noexcept -> double { return shape_->area(); }
  };
#+end_src

It works the same as before, but the wrapper logic is hidden from the
consoomer

#+begin_src cpp
  auto printAreas(const std::vector<AnyShape>& shapes) -> void {
    for (const auto& shape : shapes) {
      std::println("Area is {:.2f}", shape.area());
    }
  }

  auto main() -> int {
    auto shapes = std::vector<AnyShape>{};
    shapes.emplace_back(Square{2});
    shapes.emplace_back(Circle{1});
    printAreas(shapes);
  }
#+end_src

** Generic std::any

Both ~Shape~ and ~ShapeWrapper~ have accepted standard names: the former
is the type-erasure /concept/[fn:19] (the interface we program against),
and the latter is the /model/ (a templated wrapper that implements the
interface and forwards to a concrete type).

Let's rewrite our original type erasure example to use the standard
parlance.  Nothing needs to be changed except a few type names

#+begin_src cpp
  #include <memory>

  class Any {
    class Concept {
    public:
      virtual ~Concept() = default;
      virtual auto f() const noexcept -> double = 0;
    };

    template <typename T>
    class Model : public Concept {
      T obj_;
    public:
      explicit Model(T obj) noexcept : obj_{std::move(obj)} {}
      auto f() const noexcept -> double override { return obj_.f(); }
    };

    std::unique_ptr<Concept> obj_;

  public:
    template <typename T>
    explicit Any(T&& obj) : obj_{std::make_unique<Model<T>>(std::forward<T>(obj))} {}

    auto f() const noexcept -> double { return obj_->f(); }
  };
#+end_src

That's it!  The class ~Any~ is a simplified version of ~std::any~, which
is even used in the STL itself (namely, in ~std::function~).  But that's
for another post.

* DONE [#B] Devirtualization and Static Polymorphism
CLOSED: [2026-01-10 Sat 13:32]
:PROPERTIES:
:EXPORT_FILE_NAME: devirtualization-and-static-polymorphism
:END:
:LOGBOOK:
- State "WAIT"       from "NEXT"       [2025-12-18 Thu 20:02]
- State "NEXT"       from "DONE"       [2025-12-18 Thu 20:02]
:END:

Ever wondered why your "clean" polymorphic design underperforms in
benchmarks?  Virtual dispatch enables polymorphism, but it comes with
hidden overhead: pointer indirection, larger object layouts, and fewer
inlining opportunities.

Compilers do their best to /devirtualize/ these calls, but it isn't
always possible.  On latency-sensitive paths, it's beneficial to
manually replace dynamic dispatch with /static polymorphism/, so calls
are resolved at compile time and the abstraction has effectively zero
runtime cost.

** Virtual dispatch

Runtime polymorphism occurs when a base interface exposes a virtual
method that derived classes override.  Calls made through a ~Base&~ are
then dispatched to the appropriate override at runtime.  Under the hood,
a virtual table (~vtable~) is created /for each class/, and a pointer
(~vptr~) to the ~vtable~ is added /to each instance/.

#+begin_src plantuml :file diagram.png
  @startuml

  hide empty members

  interface Base {
    + virtual auto foo()
  }

  class Derived {
    + auto foo() override
  }

  Base <|-- Derived

  class ZZZ
  hide ZZZ

  object BaseVT <<vtable>> {
    + &Base::foo
  }

  object DerivedVT <<vtable>> {
    + &Derived::foo
  }

  object BaseObject {
    - vptr : *vtable
    - ...members...
  }

  object DerivedObject {
    - vptr : *vtable
    - ...members...
  }

  object DerivedObject2 {
    - vptr : *vtable
    - ...members...
  }

  BaseObject --> BaseVT : vptr
  DerivedObject --> DerivedVT : vptr
  DerivedObject2 --> DerivedVT : vptr

  @enduml
#+end_src

#+caption: *Virtual dispatch diagram.*  The method ~foo~ is declared virtual in ~Base~ and overridden in ~Derived~.  Both classes get a ~vtable~, and each object gets a ~vptr~ pointing to the corresponding ~vtable~.
#+RESULTS:
[[file:diagram.png]]

On a virtual call, the compiler loads the ~vptr~, selects the right slot
in the ~vtable~, and performs an indirect call through that function
pointer.  The drawback is that the extra ~vptr~ increases object size,
and the ~vtable~ makes the call hard to predict.  This prevents
inlining, increases branch mispredictions, and reduces cache efficiency.

The best way to observe this phenomenon is by inspecting the
assembly[fn:4] code emitted by the compiler for a minimal example

#+begin_src cpp
  class Base {
  public:
    auto foo() -> int;
  };

  auto bar(Base* base) -> int {
    return base->foo() + 77;
  }
#+end_src

For a non-virtual member function ~foo~ like in the example above, the
free function ~bar~ issues a direct call

#+begin_src asm
  bar(Base*):
          sub     rsp, 8
          call    Base::foo()  // Direct call
          add     rsp, 8
          add     eax, 77
          ret
#+end_src

However, declaring ~foo~ as ~virtual~ changes ~bar~'s assembly into an
indirect, vtable-based call

#+begin_src asm
  bar(Base*):
          sub     rsp, 8
          mov     rax, QWORD PTR [rdi]  // vptr (pointer to vtable)
          call    [QWORD PTR [rax]]     // Virtual call
          add     rsp, 8
          add     eax, 77
          ret
#+end_src

** Devirtualization

Sometimes the compiler can statically deduce which override a virtual
call will hit.  In those cases, it /devirtualizes/ the call and emits a
direct call instead (skipping the ~vtable~).  For example,
devirtualization is straightforward[fn:5] when the runtime type is
clearly fixed

#+begin_src cpp
  struct Base {
    virtual auto foo() -> int = 0;
  };

  struct Derived : Base {
    auto foo() -> int override { return 77; }
  };

  auto bar() -> int {
    Derived derived;
    return derived.foo();  // compiler knows this is Derived::foo
  }
#+end_src

The compiler is able to devirtualize even through a base pointer, as
long as it can track the allocation and prove there is only one possible
concrete type.  The problem is that with traditional compilation, object
files are created per translation unit (TU)---compiled and optimized in
isolation.  The linker simply stitches those objects together, so
cross-TU optimizations are inherently limited.  That's where compiler
flags are useful.

- ~-fwhole-program~ :: tells the compiler "this translation unit is the
  entire program."  If no class derives from ~Base~ in this TU, the
  compiler is free to assume nothing ever does, and can devirtualize
  calls on ~Base~.

- ~-flto~ :: link-time optimization.  Keeps an intermediate
  representation in the object files and optimizes across all of them at
  link time, effectively treating multiple source files as a single TU.

On the language side, ~final~ is a lightweight way to give the compiler
the same guarantee for specific methods

#+begin_src cpp
  class Base {
  public:
    virtual auto foo() -> int;
    virtual auto bar() -> int;
  };

  class Derived : public Base {
  public:
    auto foo() -> int override;  // override
    auto bar() -> int final;     // final
  };

  auto test(Derived* derived) -> int {
    return derived->foo() + derived->bar();
  }
#+end_src

Here, ~foo()~ can still be overridden, so ~derived->foo()~ remains a
virtual call.  However, ~bar()~ is marked as ~final~, so the compiler
emits a direct call even though it's declared ~virtual~ in the base

#+begin_src asm
  test(Derived*):
          push    rbx
          sub     rsp, 16
          mov     rax, QWORD PTR [rdi]
          mov     QWORD PTR [rsp+8], rdi
          call    [QWORD PTR [rax]]       // Virtual call
          mov     rdi, QWORD PTR [rsp+8]
          mov     ebx, eax
          call    Derived::bar()          // Direct call
          add     rsp, 16
          add     eax, ebx
          pop     rbx
          ret
#+end_src

** Static polymorphism

When the compiler can't devirtualize, one option is to use static
polymorphism instead.  The canonical tool for this is the Curiously
Recurring Template Pattern[fn:6] (CRTP).  With CRTP, the base class is
templated on the derived class, and invokes methods on it via
~static_cast~---no virtual keyword involved

#+begin_src cpp
  template <typename Derived>
  class Base {
  public:
    auto foo() -> int {
      return 77 + static_cast<Derived*>(this)->bar();
    }
  };

  class Derived : public Base<Derived> {
  public:
    auto bar() -> int {
      return 88;
    }
  };

  auto test() -> int {
    Derived derived;
    return derived.foo();
  }
#+end_src

With ~-O3~ optimization, the compiler inlines everything and
constant-folds the result.  No ~vtable~, no ~vptr~, no indirection.
Fully optimized[fn:7] call.

#+begin_src asm
  test():
          mov     eax, 165  // 77 + 88
          ret
#+end_src

*Deducing this.* C++23's /deducing this/ keeps the same static-dispatch
model but makes it easier to write.  Instead of templating the entire
class (and writing ~Base<Derived>~ everywhere), you template only the
member function that needs access to the derived type, and let the
compiler deduce ~self~ from ~*this~

#+begin_src cpp
  class Base {
  public:
    auto foo(this auto&& self) -> int { return 77 + self.bar(); }
  };

  class Derived : public Base {
  public:
    auto bar() -> int { return 88; }
  };
#+end_src

This yields identical optimized code: ~foo~ is instantiated as
~foo<Derived>~, and the call to ~bar~ is resolved statically and
inlined.

* DONE [#B] About this Site
CLOSED: [2018-05-11 Mon 21:18]
:PROPERTIES:
:EXPORT_FILE_NAME: about-this-website
:END:
:LOGBOOK:
CLOCK: [2026-01-24 Sat 10:47]--[2026-01-24 Sat 10:47] =>  0:00
:END:

After nearly a decade, I've rebuilt my personal site from scratch---a
blog on software, self-hosting, and lessons learned along the way.  This
first post is backdated to when I originally launched a site on this
domain, back in 2018.

*What?*---A personal blog.  Posts aim to be concise, practical, and
rigorous.

*Why?*---To give back to the /libre/ software community.  To keep a
record of my learning journey.  To deepen my own understanding by
writing things down.[fn:23]

*Who?*---See the About page.

*Where?*---Self-hosted at my mother's house in northern Spain (ha!),
exposed to the Internet through a WireGuard tunnel to a VPS, and served
via a global CDN.

*When?*---Once or twice a month.

*How?*---Written in Emacs Org-mode, exported to Hugo, with a custom
theme.[fn:22]

If you spot an error or have suggestions, I welcome feedback---reach out
via the contact link on the About page.

* DONE [#B] David Álvarez Rosa
CLOSED: [2026-01-24 Sat 12:31]
:PROPERTIES:
:EXPORT_FILE_NAME: about
:EXPORT_HUGO_SECTION: .
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :subtitle Curriculum Vitae
:END:

#+begin_export html
<style>
  .signature { display: none; }
  sup { display: none; }
  .side img { max-width: 225px; box-shadow: none; margin-top: -26px;}
  @media (max-width: 860px) { .side img { margin-top: 0!important; } }
</style>
#+end_export

 [fn:8]Mathematician and engineer based in sunny Dublin, passionate
about low-latency, high-performance systems.

Currently working in algorithmic trading at Susquehanna.  Previously
designed and built systems at Amazon serving 10M+ monthly active
customers, developed semantic caching for LLMs at Sopra Steria, and
conducted quantitative cybersecurity risk analysis at Deloitte.

Holds a BSc in Mathematics, a BEng in Industrial Engineering, and an MSc
in Artificial Intelligence.

** Experience

/Software Engineer @ Susquehanna/
[fn::Jul 2024--Present\\
Dublin, Ireland]\\
High-frequency options trading.  Low-latency market data and trading
signals.  Mentor and interviewer.

/Software Engineer II @ Amazon/
[fn::Mar 2022--Aug 2024\\
Madrid, Spain]\\
Designed systems for 10M+ monthly active customers.  Contributed to 100+
internal repos.  Won org hackathon.  Promoted in 18 months (top 5%).
Mentored 3.  On-call.  Interviewer.

/Machine Learning Engineer @ Sopra Steria/
[fn::Apr 2024--Jul 2024\\
Remote]\\
Researched, designed, and built a semantic cache for LLMs.

/Risk Analyst @ Deloitte/
[fn::Sep 2021--Mar 2022\\
Madrid, Spain]\\
Quantitative analysis of technological and cybersecurity risks for
top-tier banking companies.

/Visiting Researcher @ Vector Institute/
[fn::Sep 2020--Jun 2021\\
Toronto, Canada]\\
Research thesis on multimodal learning (recomprehension.com).

/Machine Learning Engineer @ BCN eMotorsport/
[fn::Sep 2019--Feb 2020\\
Barcelona, Spain]\\
Perception at Driverless UPC.  I served as LiDAR lead and collaborated
on computer vision for a fully autonomous car.

** Education

/MSc in Artificial Intelligence/
[fn::GPA 9.00/10\\
Honors in 6 subjects]\\
Official study program focused on AI research and enabling PhD.

/MSc in Mathematics/\\
Math-lover part-time student. Dropout (joined Amazon).

/Research Thesis/
[fn::GPA 10/10 (A+)]\\
Research thesis on multimodal learning at University of Toronto.

/BSc in Mathematics/
[fn::GPA 8.12/10 (top 10%)\\
Honors in 9 subjects]\\
Rigorous and proof-oriented degree with a robust mathematical base.

/BEng in Industrial Engineering/
[fn::GPA 8.03/10 (top 2%)\\
Honors in 14 subjects]\\
Multidisciplinary and integrative vision of industrial engineering.

** Licenses & certifications

Certificate in Advanced English (C1) --- Cambridge University\\
Machine Learning --- Stanford University\\
Deep Learning --- deeplearning.ai\\
Blockchain & Financial Technology --- Hong Kong University\\
Nova Talent Member --- Nova

** Volunteering

/Mathematics Tutor/\\
Academic training for the Mathematical Olympiads.

/Volunteer @ Banco de Alimentos/\\
Food collection for people in need.

** Honors & awards

/Mathematical Olympiad/\\
Silver in local (Pamplona), honors in national (Barcelona).

/Physics Olympiad/\\
Gold in local (Pamplona), silver in national (Seville).

/Mobility Scholarship --- Cellex (CFIS)/
[fn::Canceled due to Covid-19]\\
Scholarship to carry out my research thesis at Toronto (€6k).

/Tuition and Housing Scholarship --- Cellex (CFIS)/\\
University tuition and housing (€19k).

/General Scholarship --- Government of Spain/\\
Full university tuition plus an annual stipend (€11k).

** Languages

English --- Proficient\\
Spanish --- Native\\
Catalan --- Intermediate

** Contact

You can reach me at david@alvarezrosa.com (preferred) or +34 647 13
39 30.

* DONE [#B] Welcome
CLOSED: [2025-12-02 Tue 19:58]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: .
:END:

 [fn:9]Hi! This is my personal site, where I share notes on software and
self-hosting.  You'll learn how things work under the hood, and how to
make them run fast, /very/ fast.

I'm a mathematician and engineer based in sunny Dublin, passionate about
low-latency, high-performance systems.  I'm a strong advocate of free
(as in /freedom/) software, and a devoted Emacs user.

Find me on [[https://github.com/david-alvarez-rosa/][GitHub]], [[https://gitlab.com/david-alvarez-rosa][GitLab]], and [[https://linkedin.com/in/david-alvarez-rosa][LinkedIn]].

* DONE [#B] Page Not Found
CLOSED: [2025-12-13 Sat 22:23]
:PROPERTIES:
:EXPORT_FILE_NAME: 404
:EXPORT_HUGO_SECTION: .
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :subtitle " "
:END:

#+begin_export html
<style> .signature { display: none; } </style>
#+end_export

That page couldn't be found (404 error).  Try the [[/][Home page]].

* DONE [#B] Subscription
CLOSED: [2025-12-13 Sat 22:06]
:PROPERTIES:
:EXPORT_FILE_NAME: subscription
:EXPORT_HUGO_SECTION: .
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :subtitle " "
:END:

Welcome! Your subscription is confirmed.  Thanks for signing up.

You can unsubscribe anytime by emailing david@alvarezrosa.com from the
same email address you subscribed with.

* Footnotes
[fn:25] ~release~ ensures prior writes are visible to threads that
~acquire~ the same variable.  Both sides use ~relaxed~ for their own
index since no other thread writes to it.

[fn:23] As Einstein put it, /"If you can't explain it simply, you don't
understand it well enough."/ Feynman followed the same principle,
testing his understanding by teaching from first principles.

[fn:22] Inspired by the work of Edward Tufte and Matthew Butterick.

[fn:21] Alongside ~-O3~, the benchmark was compiled with ~-march=native~
and ~-ffast-math~, though these flags shouldn't make a difference here.

[fn:20] Since you're employing polymorphism in the first place, most
callers will likely fall into the second group, and will need to be
templates themselves too so they can pass the type through.  That can
quickly spread templates across the codebase, making it harder to read
and structure, increasing compile times, and producing larger binaries
with slower startup.

[fn:19] The type erasure concept is an OO-style interface (a vtable).
It's unrelated to C++20 ~concept~ (compile-time predicates).

[fn:18] This implementation always heap-allocates.  Production
~std::any~ implementations often use small buffer optimization (SBO)
techniques to store small objects inline and avoid allocation.

[fn:17] It's useful to observe the number of cache misses with ~perf
stat -e cache-misses~; they are greatly reduced in this approach.

[fn:16] This advanced optimization was initially proposed by [[https://rigtorp.se][Erik
Rigtorp]].

[fn:15] Note that we are manually aligning ~alignas~ the atomics to
ensure they fall in different cache lines (commonly 64 bytes).  This
prevents false sharing, hence optimizes CPU cache usage.

[fn:14] Compiled with ~clang~ compiler with highest ~-O3~ optimization
level, and ~-march=native -ffast-math~.  Consumer and producer threads
are pinned to dedicated cores (Intel Core Ultra 5 135U).  See [[https://github.com/david-alvarez-rosa/CppPlayground/blob/main/DataStructures/ring_buffer.cpp][benchmark]].

[fn:13] If you tried to pass in a type that doesn't conform to the
'interface' (say, ~std::string~), the compiler would hit an error when
you try to compile the method call, complaining that ~std::string~
doesn't have an ~area~ method.

[fn:12] In some cases, you may not have control of the concrete types
(e.g. think STL types like ~std::string~), or it may not even be
possible for the concrete type to inherit (e.g. builtins like ~int~).

[fn:10] Remember that interfaces that are intended to be used through a
~Base&~ or ~Base*~ must have a virtual destructor, to ensure derived
classes are properly destructed [[https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#c127-a-class-with-a-virtual-function-should-have-a-virtual-or-protected-destructor][(C.127)]].

[fn:9] [[./static/images/home-illustration.png]] ‎

[fn:8] [[./static/images/portrait.png]] *That's me!* March 2022.

[fn:11] [[./static/images/ringbuffer.jpg]] *Ring buffer with 32 slots.* The
producer has filled 15 of them, indicated by blue.  The consumer is
behind the producer, reading data from the slots, freeing them as it
does so.  A free slot is indicated by orange.

[fn:1] By using ~std::array~ we are forcing clients to define the buffer
size as ~constexpr~.  It's also common to use instead a ~std::vector~ to
remove that restriction.  A further optimization is to constrain the
capacity to a power of two, allowing wrap-around via bit masking ~head &
(N - 1)~ instead of a branch.

[fn:2] Note how one item is left unused to indicate that the queue is
full.  When ~head_~ is one item ahead of ~tail_~, the queue is full.

[fn:3] Again note that ~head_ == tail_~ indicates that the queue is
empty.

[fn:4] Assembly generated with ~gcc~ at ~-O3~ on x86-64.  Similar
results were observed with ~clang~ on the same platform.

[fn:5] The compiler emits a direct call to ~Derived::foo~ (or inlines
it), because ~derived~ cannot have any other dynamic type.

[fn:6] The curiously recurring template pattern is an idiom where a
class X derives from a class template instantiated with X itself as a
template argument.  More generally, this is known as F-bound
polymorphism, a form of F-bounded quantification.

[fn:7] The trade-off is that each ~Base<Derived>~ instantiation is a
distinct, unrelated type, so there's no common runtime base to upcast
to.  Any shared functionality that operates across different derived
types must itself be templated.
