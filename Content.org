#+title: Personal Website Content
#+author: David Álvarez Rosa
#+startup: logdone
#+filetags: :pers:blog:
#+CATEGORY: Blog

* DONE [#B] Server Setup
CLOSED: [2026-02-03 Mon 20:00]
:PROPERTIES:
:EXPORT_FILE_NAME: server-hardening
:END:

A misconfigured server is an open invitation to attackers. After
managing dozens of servers, these are the essential steps I run on every
fresh install.

** Installation and DNS

Start with a clean Linux install and a domain name. For partitioning, I
keep it simple: one large root partition plus swap.[fn::Predicting
partition sizes is hard. A single root partition simplifies management
and avoids running out of space in one mount point while others sit
empty.] While I run Arch on my personal machine, I prefer Debian for
servers because its stability and the long term support.

From your DNS registrar point your domain to the server's IP. Create an
A record for IPv4 and an AAAA record for IPv6. Wait a few minutes for
changes to propagate, then verify

#+begin_src sh
  $ dig alvarezrosa.com A +short
  213.32.19.229
  $ dig alvarezrosa.com AAAA +short
  2001:41d0:305:2100::febc
#+end_src

** First login

Log in, change the root password, and update installed packages

#+begin_src sh
  $ ssh root@alvarezrosa.com
  $ passwd
  $ apt update && apt upgrade
#+end_src

Create a non-root user with sudo privileges

#+begin_src sh
  $ useradd --create-home --groups sudo david
  $ passwd david
#+end_src

Log out and log back in as the new user[fn::From this point forward,
avoid using root directly. Running as a non-root user with sudo is safer
and prevents accidental system-wide changes.]

#+begin_src sh
  $ ssh david@alvarezrosa.com
#+end_src

** Dotfiles

I like to set up my dotfiles from the start for a familiar
environment[fn::I usually set up SSH keys for GitHub.  The commands here
treat your home directory as a git repo, which lets you track dotfiles
without symlinking.]

#+begin_src sh
  $ git init
  $ git remote add origin https://github.com/david-alvarez-rosa/dotfiles.git
  $ git fetch origin
  $ git checkout -t origin/main
  $ git submodule update --init --recursive
  $ git config status.showUntrackedFiles no
#+end_src

I configure zsh with oh-my-zsh and starship[fn::oh-my-zsh provides
plugins and themes for zsh. starship is a fast, customizable prompt that
works across shells.]

#+begin_src sh
  $ sudo apt install zsh
  $ chsh --shell $(which zsh) david
  $ sh -c "$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)"
  $ curl -sS https://starship.rs/install.sh | sh
#+end_src

Log out and back in to verify the new shell loads.

** SSH keys

Configure SSH key-based authentication.  From your local machine, copy
your public key to the server, and verify you can log in without a
password

#+begin_src sh
  $ ssh-copy-id david@alvarezrosa.com
  $ ssh david@alvarezrosa.com
#+end_src

If you need root access via SSH keys, copy the authorized keys file

#+begin_src sh
  $ su -
  $ cp /home/david/.ssh/authorized_keys ~/.ssh/authorized_keys
#+end_src

I keep password authentication enabled for my user but disabled for
root.[fn::Sometimes I SSH from computers that aren't my laptop. Disable
it entirely by setting ~PasswordAuthentication no~ in ~sshd_config~.]

** Timezone, locale, and hostname

Set your timezone, and verify with ~date~

#+begin_src sh
  $ timedatectl list-timezones
  $ sudo timedatectl set-timezone Europe/Madrid
  $ date
#+end_src

Configure locales by uncommenting en_US in ~/etc/locale.gen~, then run

#+begin_src sh
  $ sudo locale-gen
#+end_src

Set a meaningful hostname and add it to ~/etc/hosts~

#+begin_src sh
    $ sudo hostnamectl set-hostname homelab
    $ cat /etc/hosts
    127.0.0.1  localhost
    ::1        localhost  ip6-localhost  ip6-loopback
    127.0.1.1  homelab
#+end_src

** Security

Block all incoming connections by default, then allow only what you
need[fn::Open port 22 before enabling the firewall, or you'll lose SSH
access to the server.]

#+begin_src sh
  $ sudo apt install ufw
  $ sudo ufw default deny incoming
  $ sudo ufw allow 22/tcp
  $ sudo ufw enable
#+end_src

Install fail2ban[fn::fail2ban monitors logs for malicious patterns and
automatically bans offending IP addresses.]

#+begin_src sh
  $ sudo apt install fail2ban
  $ sudo systemctl enable --now fail2ban
#+end_src

Enable unattended-upgrades[fn::Automatic security updates ensure your
system stays patched without manual intervention.]

#+begin_src sh
  $ sudo apt install unattended-upgrades apt-listchanges
  $ sudo dpkg-reconfigure --priority=low unattended-upgrades
#+end_src

** Web server

I usually install a web server early on to verify everything
works[fn::nginx is a lightweight, high-performance web server. I used
to use Apache, but nginx has better performance.]

#+begin_src sh
  $ sudo apt install nginx
  $ sudo systemctl enable --now nginx
  $ sudo ufw allow 80/tcp
#+end_src

Verify by navigating to http://alvarezrosa.com from the browser.  Then
secure the site with HTTPs[fn::Certbot automatically obtains and
installs free TLS certificates from Let's Encrypt. It also sets up
automatic renewal via a systemd timer.]

#+begin_src sh
  $ sudo ufw allow 443/tcp
  $ sudo apt install certbot python3-certbot-nginx
  $ sudo certbot
#+end_src

Follow the prompts to configure HTTPS. Then, go back to the browser and
verify https://alvarezrosa.com loads correctly.

* TODO [#B] Selfhost an email server guide
Email guide:

1. Receiving email
2. Sending email
3. Connecting clients (IMAP only)
4. rDNS, SPF, DKIM, DMARC
5. Multiple domains


** Receiving email

1. Configure DNS records A/AAAA for mail.alvarezmagan.com, wait for them
   to propagate, and validate. Also DNS MX (Mail Exchanger) records ~MX
   10 mail.alvarezmagan.com.~ Check with
   #+begin_src
     $ dig +short A mail.alvarezmagan.com
    213.32.19.229
    $ dig +short AAAA mail.alvarezmagan.com
    2001:41d0:305:2100::febc
     $ dig +short MX alvarezmagan.com
       10 mail.alvarezmagan.com
   #+end_src
1. Hostname can be whatever, but ensure that in /etc/hosts it points
   to mail.alvarezmagan.com
   #+begin_src sh
     sudo vim /etc/hosts
        127.0.1.1       mail.alvarezmagan.com        homelab
$ hostname -f
  mail.alvarezmagan.com
   #+end_src
5. Install Postfix, enter internet site with mail name
   mail.alvarezmagan.com, and open port 25
   #+begin_src sh
     apt install postfix
     ufw allow 25/tcp
   #+end_src
8. From your computer should already be able to telnet potfix
   #+begin_src sh
     $ telnet mail.alvarezmagan.com 25
       Trying 213.32.19.229...
       Connected to mail.alvarezmagan.com.
       220 mail.alvarezmagan.com ESMTP Postfix (Debian)
   #+end_src
9. Add domain to /etc/postfix/main.cf mydestination
   #+begin_src sh
   mydestination = $myhostname, mail.alvarezmagan.com, localhost.alvarezmagan.com, , localhost, alvarezmagan.com
   #+end_src
   Restart ~systemctl restart postfix~
10. The server is ready to receive email! Sent an email to your account
    david@alvarezmagan.com from. check it
    #+begin_src
      # apt install mailutils
      $ mail
 Mail version 8.1.2 01/15/2001.  Type ? for help.
 "/var/mail/david": 1 message 1 unread
 >U  1 david@alvarezrosa  Mon Feb  2 21:09   33/1292  Hola hola
    #+end_src

** Sending email

11. Update postfix conf in /etc/postfix/main.cf
    #+begin_src
      mydomain = alvarezmagan.com
      myorigin = $mydomain
    #+end_src
    restart, ~systemctl restart postfix~
12. You are ready to send email to any address you like
    #+begin_src sh
      echo test | mail --subject "test 4" --append "From: david@alvarezmagan.com" david@alvarezrosa.com
    #+end_src

** Connecting clients

13. Now we expose mailboxes via IMAP so clients can read mail
    #+begin_src
      # apt install dovecot-core dovecot-imapd
    #+end_src
14. Get certificate for fqdn imap
        #+begin_src
          # certbot certonly -d mail.alvarezmagan.com
        #+end_src
15. Configure Dovecot TLS: =/etc/dovecot/conf.d/10-ssl.conf= Set:
     #+begin_src
     ssl = required
     ssl_cert = /etc/letsencrypt/live/mail.alvarezmagan.com/fullchain.pem
     ssl_key = /etc/letsencrypt/live/mail.alvarezmagan.com/privkey.pem
     #+end_src
16. Uncomment lines =/etc/postfix/master.cf=
  #+begin_src conf
submission inet n       -       y       -       -       smtpd
  -o syslog_name=postfix/submission
  -o smtpd_tls_security_level=encrypt
  -o smtpd_sasl_auth_enable=yes
  -o smtpd_recipient_restrictions=permit_sasl_authenticated,reject
  -o milter_macro_daemon_name=ORIGINATING
  #+end_src
17. Open firewall.   - *587*: SMTP submission (recommended for clients;
    typically STARTTLS), and   - *993*: IMAP over TLS/SSL
    #+begin_src
# ufw allow 587/tcp
# ufw allow 993/tcp
    #+end_src
18. Restart services
    #+begin_src
# systemctl restart dovecot postfix
    #+end_src
19. Edit /etc/dovecot/conf.d/10-master.conf. Find =service auth {
    ... }= and ensure you have: (just uncomment)
    #+begin_src sh
      service auth {
        unix_listener /var/spool/postfix/private/auth {
          mode = 0660
        }
      }
    #+end_src
21. use thunderbird / mutt or mu4e to check. thunderbird is nice for
    testing (only for testing), because it autofills some fields for you

** rDNS, SPF, DKIM, DMARC

rDNS

1. Configure rDNS from the VPS provider from "homelab" host back to
   DNS. Validate that it's working:
   #+begin_src sh
     $ dig +short -x 213.32.19.229
       mail.alvarezmagan.com.
      $ dig +short -x 2001:41d0:305:2100::febc
      mail.alvarezmagan.com.
   #+end_src

SPF

2. Setup SPF Sender Policy Framework, and validate.  =v=spf1 mx -all=
   means only the servers listed in your domain’s MX records are allowed
   to send email for the domain, and all other senders should fail SPF.
   #+begin_src
     $ dig +short TXT alvarezmagan.com
     v=spf1 mx -all
   #+end_src

DMARC

3. Set up DMARC by publishing a DNS TXT record at
   *_dmarc.yourdomain.com* that tells receivers what to do when SPF/DKIM
   checks (and alignment) fail. It’s a DMARC policy that tells receivers
   to reject mail that fails DMARC checks and to send aggregate reports
   to david@yourdomain.com.
   #+begin_src
     v=DMARC1; p=reject; rua=mailto:david@alvarezmagan.com
     $ dig +short TXT _dmarc.alvarezmagan.com
   #+end_src

DKIM

1. Install opendkim
   #+begin_src
     apt install opendkim opendkim-tools
   #+end_src
2. generate keys
   #+begin_src
     sudo mkdir -p /etc/opendkim/keys/alvarezmagan.com
sudo opendkim-genkey -D /etc/opendkim/keys/alvarezmagan.com -d alvarezmagan.com -s mail
sudo chown -R opendkim:opendkim /etc/opendkim/keys
sudo chmod 600 /etc/opendkim/keys/alvarezmagan.com/mail.private
   #+end_src
3. Create a DNS TXT record, - *Name:*
   =mail._domainkey.alvarezmagan.com=, - *Value:* the =v=DKIM1; ... p=...= from that file (as one TXT value)
   #+begin_src
     sudo cat /etc/opendkim/keys/alvarezmagan.com/mail.txt
   #+end_src
   Validate it has been created
   #+begin_src
     dig +short TXT mail._domainkey.alvarezmagan.com
   #+end_src
4. Configure opendkim
   #+begin_src
     ❯ sudo cat /etc/opendkim.conf
Mode            sv
Domain          alvarezmagan.com
Selector        mail
KeyFile         /etc/opendkim/keys/alvarezmagan.com/mail.private
Socket          inet:12301@localhost
UserID opendkim
PidFile /run/opendkim/opendkim.pid
   #+end_src
5. hook postfix to opendkim   Add to =/etc/postfix/main.cf=:
  #+begin_src conf
milter_default_action = accept
milter_protocol = 6
smtpd_milters = inet:localhost:12301
non_smtpd_milters = inet:localhost:12301
  #+end_src
  Restart ~systemctl restart postfix~
6. Validate by sending email to gmail, check headers, and also
   mail-tester.com to get score 10/10 and also testemailserver from
   mxtoolbox.com and get everything green

* TODO [#B] Template Argument Deduction                             :backlog:

- https://en.cppreference.com/w/cpp/language/template_argument_deduction.html
- Perfect forwarding

* TODO [#B] Value Categories                                        :backlog:

- https://en.cppreference.com/w/cpp/language/value_category.html
- https://0xghost.dev/blog/std-move-deep-dive/

* TODO [#B] How to setup a tunnel from vps to local homelab         :backlog:
:PROPERTIES:
:EXPORT_FILE_NAME: pending-tunnel
:END:

Write a blog on how to setup a tunnel from vps to local homelab

How to manage a selfhosted homelab behind CGNAT?

Homelab <-> Router <-> NAT <-> ONT <-> *CGNAT* <-> ISP <-> Internet

Expose the server through public VPS

Setting up a VPN tunnel from VPS to local homelab

Server
#+begin_src conf
  [Interface]
  Address = 10.0.0.1/24
  PrivateKey = <server-private-key>
  ListenPort = 51820
  MTU = 1280
  PostUp = iptables -t nat -A PREROUTING -i ens6 -p udp --dport 51820 -j RETURN
  PostUp = iptables -t nat -A PREROUTING -i ens6 -j DNAT --to-destination 10.0.0.2
  PostUp = iptables -A FORWARD -i wg0 -o ens6 -s 10.0.0.2 -j ACCEPT
  PostUp = iptables -t nat -A POSTROUTING -j MASQUERADE
  PostDown = iptables -t nat -D PREROUTING -i ens6 -p udp --dport 51820 -j RETURN
  PostDown = iptables -t nat -D PREROUTING -i ens6 -j DNAT --to-destination 10.0.0.2
  PostDown = iptables -D FORWARD -i wg0 -o ens6 -s 10.0.0.2 -j ACCEPT
  PostDown = iptables -t nat -D POSTROUTING -j MASQUERADE

  [Peer]
  PublicKey = <homelab-public-key>
  AllowedIPs = 10.0.0.2/32
#+end_src

Homelab
#+begin_src conf
  [Interface]
  Address = 10.0.0.2/24
  PrivateKey = <homelab-private-key>

  [Peer]
  PublicKey = <server-public-key>
  Endpoint = <server-public-ip>:51820
  AllowedIPs = 10.0.0.1/32
  PersistentKeepalive = 25
#+end_src

Now the proxy all stuff with iptables (DNAT)
#+begin_src sh
  sysctl -w net.ipv4.ip_forward=1
  echo net.ipv4.ip_forward=1 >/etc/sysctl.d/99-forward.conf  # Persist config
  sysctl -w net.ipv4.conf.ens6.route_localnet=1  # Change ens6 with eth0 or whatever
#+end_src

What happens if VPS is not working?

Setup an alternate way of accessing the homelab

- Port forwarding + DynDNS: not possible behind CGNAT, but good option
  otherwise
- Use a third party: Tailscale/Cloudflare, etc. On my case, I've setup
  cloudflare one zero trust.  It uses outbound service daemon on both
  client and server, so no need to expose anything (same as vps bastion
  or jump box). need to run cloudflared both in server and client

What happens if homelab is not working?

In that case, you'll like to reboot, but if you cannot access it on the
first place that's a big problem

- Manually restarted if you are physically close to the server
- Setup heuristic cronjobs: check if the server is accessible by itself
  once in a while, and if that fails, then it will restart and notify

* TODO [#B] Incremental clang-tidy tool                             :backlog:
:PROPERTIES:
:EXPORT_FILE_NAME: pending-incremental
:END:

How to incrementally run clang-tidy or cppcheck

* TODO [#B] Spin-Lock                                               :backlog:
:PROPERTIES:
:EXPORT_FILE_NAME: spin-lock
:END:

Pending. Optimizing a spin-lock using this youtube series.
Might be wrong with the right youtube series bla bla bla more text

https://www.youtube.com/watch?v=AN6XHy2znzc

Pending.

* TODO [#B] SFINAE                                                  :backlog:

Pending.

* TODO [#B] Lru Cache                                               :backlog:

Pending.

* TODO [#B] Small buffer optimization                               :backlog:

Specially small string optimization.

* TODO [#B] Optimizing Matrix Multiplication                        :backlog:

Fastest matrix multiplication.

* TODO [#B] Translation Look Aside Buffer                           :backlog:
:PROPERTIES:
:EXPORT_FILE_NAME: translation-look-aside-buffer
:END:

Explain what the TLB is, using maybe hrt blog?

* TODO [#B] Vector push_back                                        :backlog:

In-depth vector push_back following guide

* TODO [#B] Implementing a Shared_ptr                               :backlog:

Implement a shared_ptr

* TODO [#B] Exploring CPU Caches                                    :backlog:
:PROPERTIES:
:EXPORT_FILE_NAME: exploring-cpu-caches
:END:

Pending.

* DONE [#B] Optimizing a Lock-Free Ring Buffer
CLOSED: [2026-01-19 Sat 13:32]
:PROPERTIES:
:EXPORT_FILE_NAME: optimizing-a-lock-free-ring-buffer
:END:
:LOGBOOK:
CLOCK: [2026-01-24 Sat 10:47]--[2026-01-24 Sat 11:35] =>  0:48
CLOCK: [2026-01-12 Mon 19:13]--[2026-01-21 Wed 19:32] => 216:19
:END:

A single-producer single-consumer (SPSC) queue is a great example of how
far constraints can take a design.  In this post, you will learn how to
implement a ring buffer from scratch: start with the simplest design,
make it thread-safe, and then gradually remove overhead while preserving
FIFO behavior and predictable latency.  This pattern is widely used to
share data between threads in the lowest-latency environments.

** What is a ring buffer?

 [fn:11]You might have run into the term circular buffer, or perhaps
cyclic queue.  These are simply other names for a /ring buffer:/ a queue
where a producer generates data and inserts it into the buffer, and a
consumer later pulls it back out, in first-in-first-out order.

What makes a ring buffer distinctive is how it stores data and the
constraints it enforces.  It has a fixed capacity; it neither expands
nor shrinks.  As a result, when the buffer fills up, the producer must
either wait until space becomes available or overwrite entries that have
not been read yet, depending on what the application expects.

The consumer's job is straightforward: read items as they arrive.  When
the ring buffer is empty, the consumer has to block, spin, or move on to
other work.  Each successful read releases a slot the producer can
reuse.  In the ideal case, the producer stays just a bit ahead, and the
system turns into a quiet game of /"catch me if you can,"/ with minimal
waiting on both sides.

** Single-threaded ring buffer

Let's start with a single-threaded ring buffer, which is just an
array[fn:1] and two indices.  We leave one slot permanently unused to
distinguish "full" from "empty."  Push writes to head and advances it;
pop reads from tail and advances it.

#+begin_src cpp
  template <typename T, std::size_t N>
  class RingBufferV1 {
    std::array<T, N> buffer_;
    std::size_t head_{0};
    std::size_t tail_{0};
  };
#+end_src

We can now implement the ~push~ (or write) operation[fn:2]

#+begin_src cpp
  auto push(const T& value) noexcept -> bool {
    auto new_head = head_ + 1;
    if (new_head == buffer_.size()) [[unlikely]] {  // Wrap-around
      new_head = 0;
    }
    if (new_head == tail_) [[unlikely]] {  // Full
      return false;
    }
    buffer_[head_] = value;
    head_ = new_head;
    return true;
  }
#+end_src

Next we implement the ~pop~ (or read) operation[fn:3]

#+begin_src cpp
  auto pop(T& value) noexcept -> bool {
    if (head_ == tail_) [[unlikely]] {  // Empty
      return false;
    }
    value = buffer_[tail_];
    auto next_tail = tail_ + 1;
    if (next_tail == buffer_.size()) [[unlikely]] {  // Wrap-around
      next_tail = 0;
    }
    tail_ = next_tail;
    return true;
  }
#+end_src

** Thread-safe ring buffer

You probably already noticed that the previous version is not
thread-safe.  The easiest way to solve this is to add a ~mutex~ around
push and pop.

#+begin_src cpp
  template <typename T, std::size_t N>
  class RingBufferV2 {
    std::mutex mutex_;

    auto push(const T& value) noexcept -> bool {
      auto lock = std::lock_guard<std::mutex>{mutex_};  // Thread-safe
      // ...
    }

    auto pop(T& value) noexcept -> bool {
      auto lock = std::lock_guard<std::mutex>{mutex_};  // Thread-safe
      // ...
    }
  };
#+end_src

It's correct and often fast enough: around *12M ops/s*[fn:14] on
consoomer hardware.  However, it pays for mutual exclusion even though
the producer and consumer never write the same index.  The ownership is
asymmetric: the producer is the only writer of head, and the consumer is
the only writer of tail.  That asymmetry is the lever to remove locks.

** Lock-free ring buffer

We can remove the locks by using atomics instead[fn:15]

#+begin_src cpp
  template <typename T, std::size_t N>
  class RingBufferV3 {
    std::array<T, N> buffer_;
    alignas(std::hardware_destructive_interference_size) std::atomic_size_t head_{0};
    alignas(std::hardware_destructive_interference_size) std::atomic_size_t tail_{0};
  };
#+end_src

The push implementation becomes

#+begin_src cpp
  auto push(const T& value) noexcept -> bool {
    const auto head = head_.load();
    auto next_head = head + 1;
    if (next_head == buffer_.size()) [[unlikely]] {
      next_head = 0;
    }
    if (next_head == tail_.load()) [[unlikely]] {
      return false;
    }
    buffer_[head] = value;
    head_.store(next_head);
    return true;
  }
#+end_src

And the pop implementation

#+begin_src cpp
  auto pop(T& value) noexcept -> bool {
    const auto tail = tail_.load();
    if (tail == head_.load()) [[unlikely]] {
      return false;
    }
    value = buffer_[tail];
    auto next_tail = tail + 1;
    if (next_tail == buffer_.size()) [[unlikely]] {
      next_tail = 0;
    }
    tail_.store(next_tail);
    return true;
  }
#+end_src

Simply removing the locks yields *35M ops/s*, more than double the
throughput of the locked version!  You have probably noticed that we are
using the default ~std::memory_order_seq_cst~ memory order for loading /
storing the atomics, which is the slowest.  Let's manually tune the
memory order[fn:25]

#+begin_src cpp
  auto push(const T& value) noexcept -> bool {
    const auto head = head_.load(std::memory_order_relaxed);
    auto next_head = head + 1;
    if (next_head == buffer_.size()) [[unlikely]] {
      next_head = 0;
    }
    if (next_head == tail_.load(std::memory_order_acquire)) [[unlikely]] {
      return false;
    }
    buffer_[head] = value;
    head_.store(next_head, std::memory_order_release);
    return true;
  }

  auto pop(T& value) noexcept -> bool {
    const auto tail = tail_.load(std::memory_order_relaxed);
    if (tail == head_.load(std::memory_order_acquire)) [[unlikely]] {
      return false;
    }
    value = buffer_[tail];
    auto next_tail = tail + 1;
    if (next_tail == buffer_.size()) [[unlikely]] {
      next_tail = 0;
    }
    tail_.store(next_tail, std::memory_order_release);
    return true;
  }
#+end_src

Rerunning the benchmark now gives an astonishing *108M ops/s*---3x the
previous version and 9x the original locked version.  Worth the effort,
right?

** Further optimization

We already have a fast ring buffer, but we can push it further.  The
main slowdown comes from the reader and writer constantly touching each
other's indexes.  That makes the CPU bounce cache lines[fn:17] between cores,
which is expensive.

To reduce this, the reader can keep a local cached copy[fn:16] of the write
index, and the writer keeps a local cached copy of the read index.  Then
they don't need to re-check the other side on every single operation:
only once in a while.

#+begin_src cpp
  template <typename T, std::size_t N>
  class RingBufferV5 {
    std::array<T, N> buffer_;
    alignas(std::hardware_destructive_interference_size) std::atomic_size_t head_{0};
    alignas(std::hardware_destructive_interference_size) std::size_t head_cached_{0};
    alignas(std::hardware_destructive_interference_size) std::atomic_size_t tail_{0};
    alignas(std::hardware_destructive_interference_size) std::size_t tail_cached_{0};
  };
#+end_src

The push operation is updated to first consult the cached tail
~tail_cached_~ and if that fails retry after updating the cache

#+begin_src cpp
  if (next_head == tail_cached_) [[unlikely]] {
    tail_cached_ = tail_.load(std::memory_order_acquire);
    if (next_head == tail_cached_) {
      return false;
    }
  }
#+end_src

The pop operation is updated in a similar way to first consult the
cached head

#+begin_src cpp
  if (tail == head_cached_) [[unlikely]] {
    head_cached_ = head_.load(std::memory_order_acquire);
    if (tail == head_cached_) {
      return false;
    }
  }
#+end_src

Throughput is now *305M ops/s*---nearly 3x faster than the manually
tuned lock-free version and 25x faster than the original locking
approach.

** Summary

If you want to reproduce these results, run the included [[https://github.com/david-alvarez-rosa/CppPlayground/blob/main/DataStructures/ring_buffer.cpp][benchmark]]
compiled with at least ~-O3~ optimization level.[fn:21] The benchmark
pins the producer and consumer threads to dedicated CPU cores to
minimize scheduling noise.

| Version | Throughput | Notes                                            |
|---------+------------+--------------------------------------------------|
|       1 | N/A        | Not thread-safe                                  |
|       2 | 12M ops/s  | Mutex / lock                                     |
|       3 | 35M ops/s  | Lock-free (atomics)                              |
|       4 | 108M ops/s | Lock-free (atomics) + memory order               |
|       5 | 305M ops/s | Lock-free (atomics) + memory order + index cache |

Long live lock-free and wait-free data structures!

* DONE [#B] Deriving Type Erasure
CLOSED: [2026-01-24 Sat 20:06]
:PROPERTIES:
:EXPORT_FILE_NAME: deriving-type-erasure
:END:
:LOGBOOK:
CLOCK: [2026-01-24 Sat 18:10]--[2026-01-24 Sat 20:06] =>  1:56
CLOCK: [2026-01-24 Sat 10:47]--[2026-01-24 Sat 10:47] =>  0:00
:END:

Ever looked at ~std::any~ and wondered what's actually going on behind
the scenes?  Beneath the intimidating interface is a clean case of type
erasure: concrete types hidden behind a small, uniform wrapper.

Starting from familiar tools---virtual functions and templates---we'll
build a minimal ~std::any~.  Along the way, type erasure shifts from
buzzword to practical technique you can recognize and reuse in your own
designs.

** Polymorphism with interfaces

The typical way to achieve polymorphism is to define an interface
consisting of pure-virtual methods you want to be able to call.  Then,
for each implementation that you want to use polymorphically, you create
a subclass that inherits from the base class and implement those
methods.

As an example, let's implement shape classes that have an ~area()~
method.  We start with an interface[fn:10] class

#+begin_src cpp
  class Shape {
  public:
    virtual ~Shape() = default;
    virtual auto area() const noexcept -> double = 0;
  };
#+end_src

And add a couple of concrete implementations for ~Square~ and ~Circle~

#+begin_src cpp
  class Square : public Shape {
    int side_;
  public:
    explicit Square(int side) noexcept : side_{side} {}
    auto area() const noexcept -> double override { return side_ * side_; }
  };

  class Circle : public Shape {
    int radius_;
  public:
    explicit Circle(int radius) noexcept : radius_{radius} {}
    auto area() const noexcept -> double override {
      return std::numbers::pi * radius_ * radius_;
    }
  };
#+end_src

Now, we can use these implementations generically, by coding against the
interface

#+begin_src cpp
  auto printArea(const Shape& shape) -> void {
    std::println("Area is {:.2f}", shape.area());
  }
#+end_src

Simple enough, right?

** Polymorphism with templates

Inheritance is a good solution to problems that require polymorphism,
but sometimes the concrete types you want to handle polymorphically
cannot share a common base class.[fn:12] In that case, if the types
provide the same interface, you can use a template to get polymorphism
instead

#+begin_src cpp
  auto printArea(const auto& shape) -> void {
    std::println("Area is {:.2f}", shape.area());
  }
#+end_src

You can use this method with ~Square~, ~Circle~, or any type that
provides a zero-argument ~area()~ returning ~double~. Templates work
because the compiler generates a version of the function for each
concrete type you use, and the call is valid as long as that generated
code would compile[fn:13] for the given type.

Unfortunately, template-based polymorphism has two main downsides.

*First,* templates do not give you one shared runtime base type like
~Shape~. Each instantiation is a distinct type, so there is no common
type for a homogeneous container; you cannot store a mix of ~Square~ and
~Circle~ in one array and handle them uniformly the way you can with a
pointer to base technique

#+begin_src cpp
  auto shapes = std::vector< ??? >{&square, &circle};
#+end_src

The *second* drawback[fn:20] is a little more subtle.  Anybody who uses
the template-based ~area(const auto&)~ method must either explicitly
specify the concrete type, or be a template itself, to pass along the
template type of ~area()~.

** Deriving std::any

Imagine ~Square~ and ~Circle~ are fixed types with no shared base class,
and you cannot change them to inherit from one.  But you still want to
handle them through a single common interface.

One way to do that is to introduce wrappers.  Define your own ~Shape~
interface, then create wrapper classes that inherit from ~Shape~ and
contain a ~Square~ or ~Circle~; each wrapper implements the virtual
methods by simply forwarding calls to the wrapped object

#+begin_src cpp
  class SquareWrapper : public Shape {
    Square square_;
  public:
    explicit SquareWrapper(Square square) noexcept : square_{std::move(square)} {}
    auto area() const noexcept -> double override { return square_.area(); }
  };

  class CircleWrapper : public Shape {
    Circle circle_;
  public:
    explicit CircleWrapper(Circle circle) noexcept : circle_{std::move(circle)} {}
    auto area() const noexcept -> double override { return circle_.area(); }
  };
#+end_src

Now we can work directly with instances of ~Shape~

#+begin_src cpp
  auto printAreas(const std::vector<std::unique_ptr<Shape>>& shapes) -> void {
    for (const auto& shape : shapes) {
      std::println("Area is {:.2f}", shape->area());
    }
  }

  auto main() -> int {
    auto shapes = std::vector<std::unique_ptr<Shape>>{};
    shapes.emplace_back(std::make_unique<SquareWrapper>(Square{2}));
    shapes.emplace_back(std::make_unique<CircleWrapper>(Circle{1}));
    printAreas(shapes);
  }
#+end_src

This approach works, but it has an obvious downside: you need a separate
wrapper type (like ~CircleWrapper~) for every concrete type you want to
adapt (like ~Circle~), which quickly turns into a pile of
boilerplate. Luckily, templates can offload much of that work to the
compiler by generating the needed code for each type automatically

#+begin_src cpp
  template <typename T>
  class ShapeWrapper : public Shape {
    T shape_;
  public:
    explicit ShapeWrapper(T shape) noexcept : shape_{std::move(shape)} {}
    auto area() const noexcept -> double override { return shape_.area(); }
  };
#+end_src

What we built above is the basis of the "type erasure" idiom.  All
that's left is to hide all this machinery behind another class, so that
callers don't have to deal with our custom interfaces and
templates[fn:18]

#+begin_src cpp
  class AnyShape {
    class Shape {  // The interface
    public:
      virtual ~Shape() = default;
      virtual auto area() const noexcept -> double = 0;
    };

    template <typename T>
    class ShapeWrapper : public Shape {  // The wrappers
      T shape_;

    public:
      explicit ShapeWrapper(T shape) noexcept : shape_{std::move(shape)} {}
      auto area() const noexcept -> double override { return shape_.area(); }
    };

    std::unique_ptr<Shape> shape_;

  public:
    template <typename T>
    explicit AnyShape(T&& shape)
        : shape_{std::make_unique<ShapeWrapper<T>>(std::forward<T>(shape))} {}

    auto area() const noexcept -> double { return shape_->area(); }
  };
#+end_src

It works the same as before, but the wrapper logic is hidden from the
consoomer

#+begin_src cpp
  auto printAreas(const std::vector<AnyShape>& shapes) -> void {
    for (const auto& shape : shapes) {
      std::println("Area is {:.2f}", shape.area());
    }
  }

  auto main() -> int {
    auto shapes = std::vector<AnyShape>{};
    shapes.emplace_back(Square{2});
    shapes.emplace_back(Circle{1});
    printAreas(shapes);
  }
#+end_src

** Generic std::any

Both ~Shape~ and ~ShapeWrapper~ have accepted standard names: the former
is the type-erasure /concept/[fn:19] (the interface we program against),
and the latter is the /model/ (a templated wrapper that implements the
interface and forwards to a concrete type).

Let's rewrite our original type erasure example to use the standard
parlance.  Nothing needs to be changed except a few type names

#+begin_src cpp
  #include <memory>

  class Any {
    class Concept {
    public:
      virtual ~Concept() = default;
      virtual auto f() const noexcept -> double = 0;
    };

    template <typename T>
    class Model : public Concept {
      T obj_;
    public:
      explicit Model(T obj) noexcept : obj_{std::move(obj)} {}
      auto f() const noexcept -> double override { return obj_.f(); }
    };

    std::unique_ptr<Concept> obj_;

  public:
    template <typename T>
    explicit Any(T&& obj) : obj_{std::make_unique<Model<T>>(std::forward<T>(obj))} {}

    auto f() const noexcept -> double { return obj_->f(); }
  };
#+end_src

That's it!  The class ~Any~ is a simplified version of
~std::any~,[fn:24] which is even used in the STL itself (namely, in
~std::function~).  But that's for another post.

* DONE [#B] Devirtualization and Static Polymorphism
CLOSED: [2026-01-10 Sat 13:32]
:PROPERTIES:
:EXPORT_FILE_NAME: devirtualization-and-static-polymorphism
:END:
:LOGBOOK:
- State "WAIT"       from "NEXT"       [2025-12-18 Thu 20:02]
- State "NEXT"       from "DONE"       [2025-12-18 Thu 20:02]
:END:

Ever wondered why your "clean" polymorphic design underperforms in
benchmarks?  Virtual dispatch enables polymorphism, but it comes with
hidden overhead: pointer indirection, larger object layouts, and fewer
inlining opportunities.

Compilers do their best to /devirtualize/ these calls, but it isn't
always possible.  On latency-sensitive paths, it's beneficial to
manually replace dynamic dispatch with /static polymorphism/, so calls
are resolved at compile time and the abstraction has effectively zero
runtime cost.

** Virtual dispatch

Runtime polymorphism occurs when a base interface exposes a virtual
method that derived classes override.  Calls made through a ~Base&~ are
then dispatched to the appropriate override at runtime.  Under the hood,
a virtual table (~vtable~) is created /for each class/, and a pointer
(~vptr~) to the ~vtable~ is added /to each instance/.

#+begin_src plantuml :file diagram.png
  @startuml

  hide empty members

  interface Base {
    + virtual auto foo()
  }

  class Derived {
    + auto foo() override
  }

  Base <|-- Derived

  class ZZZ
  hide ZZZ

  object BaseVT <<vtable>> {
    + &Base::foo
  }

  object DerivedVT <<vtable>> {
    + &Derived::foo
  }

  object BaseObject {
    - vptr : *vtable
    - ...members...
  }

  object DerivedObject {
    - vptr : *vtable
    - ...members...
  }

  object DerivedObject2 {
    - vptr : *vtable
    - ...members...
  }

  BaseObject --> BaseVT : vptr
  DerivedObject --> DerivedVT : vptr
  DerivedObject2 --> DerivedVT : vptr

  @enduml
#+end_src

#+caption: *Virtual dispatch diagram.*  The method ~foo~ is declared virtual in ~Base~ and overridden in ~Derived~.  Both classes get a ~vtable~, and each object gets a ~vptr~ pointing to the corresponding ~vtable~.
#+RESULTS:
[[file:diagram.png]]

On a virtual call, the compiler loads the ~vptr~, selects the right slot
in the ~vtable~, and performs an indirect call through that function
pointer.  The drawback is that the extra ~vptr~ increases object size,
and the ~vtable~ makes the call hard to predict.  This prevents
inlining, increases branch mispredictions, and reduces cache efficiency.

The best way to observe this phenomenon is by inspecting the
assembly[fn:4] code emitted by the compiler for a minimal example

#+begin_src cpp
  class Base {
  public:
    auto foo() -> int;
  };

  auto bar(Base* base) -> int {
    return base->foo() + 77;
  }
#+end_src

For a non-virtual member function ~foo~ like in the example above, the
free function ~bar~ issues a direct call

#+begin_src asm
  bar(Base*):
          sub     rsp, 8
          call    Base::foo()  // Direct call
          add     rsp, 8
          add     eax, 77
          ret
#+end_src

However, declaring ~foo~ as ~virtual~ changes ~bar~'s assembly into an
indirect, vtable-based call

#+begin_src asm
  bar(Base*):
          sub     rsp, 8
          mov     rax, QWORD PTR [rdi]  // vptr (pointer to vtable)
          call    [QWORD PTR [rax]]     // Virtual call
          add     rsp, 8
          add     eax, 77
          ret
#+end_src

** Devirtualization

Sometimes the compiler can statically deduce which override a virtual
call will hit.  In those cases, it /devirtualizes/ the call and emits a
direct call instead (skipping the ~vtable~).  For example,
devirtualization is straightforward[fn:5] when the runtime type is
clearly fixed

#+begin_src cpp
  struct Base {
    virtual auto foo() -> int = 0;
  };

  struct Derived : Base {
    auto foo() -> int override { return 77; }
  };

  auto bar() -> int {
    Derived derived;
    return derived.foo();  // compiler knows this is Derived::foo
  }
#+end_src

The compiler is able to devirtualize even through a base pointer, as
long as it can track the allocation and prove there is only one possible
concrete type.  The problem is that with traditional compilation, object
files are created per translation unit (TU)---compiled and optimized in
isolation.  The linker simply stitches those objects together, so
cross-TU optimizations are inherently limited.  That's where compiler
flags are useful.

- ~-fwhole-program~ :: tells the compiler "this translation unit is the
  entire program."  If no class derives from ~Base~ in this TU, the
  compiler is free to assume nothing ever does, and can devirtualize
  calls on ~Base~.

- ~-flto~ :: link-time optimization.  Keeps an intermediate
  representation in the object files and optimizes across all of them at
  link time, effectively treating multiple source files as a single TU.

On the language side, ~final~ is a lightweight way to give the compiler
the same guarantee for specific methods

#+begin_src cpp
  class Base {
  public:
    virtual auto foo() -> int;
    virtual auto bar() -> int;
  };

  class Derived : public Base {
  public:
    auto foo() -> int override;  // override
    auto bar() -> int final;     // final
  };

  auto test(Derived* derived) -> int {
    return derived->foo() + derived->bar();
  }
#+end_src

Here, ~foo()~ can still be overridden, so ~derived->foo()~ remains a
virtual call.  However, ~bar()~ is marked as ~final~, so the compiler
emits a direct call even though it's declared ~virtual~ in the base

#+begin_src asm
  test(Derived*):
          push    rbx
          sub     rsp, 16
          mov     rax, QWORD PTR [rdi]
          mov     QWORD PTR [rsp+8], rdi
          call    [QWORD PTR [rax]]       // Virtual call
          mov     rdi, QWORD PTR [rsp+8]
          mov     ebx, eax
          call    Derived::bar()          // Direct call
          add     rsp, 16
          add     eax, ebx
          pop     rbx
          ret
#+end_src

** Static polymorphism

When the compiler can't devirtualize, one option is to use static
polymorphism instead.  The canonical tool for this is the Curiously
Recurring Template Pattern[fn:6] (CRTP).  With CRTP, the base class is
templated on the derived class, and invokes methods on it via
~static_cast~---no virtual keyword involved

#+begin_src cpp
  template <typename Derived>
  class Base {
  public:
    auto foo() -> int {
      return 77 + static_cast<Derived*>(this)->bar();
    }
  };

  class Derived : public Base<Derived> {
  public:
    auto bar() -> int {
      return 88;
    }
  };

  auto test() -> int {
    Derived derived;
    return derived.foo();
  }
#+end_src

With ~-O3~ optimization, the compiler inlines everything and
constant-folds the result.  No ~vtable~, no ~vptr~, no indirection.
Fully optimized[fn:7] call.

#+begin_src asm
  test():
          mov     eax, 165  // 77 + 88
          ret
#+end_src

*Deducing this.* C++23's /deducing this/ keeps the same static-dispatch
model but makes it easier to write.  Instead of templating the entire
class (and writing ~Base<Derived>~ everywhere), you template only the
member function that needs access to the derived type, and let the
compiler deduce ~self~ from ~*this~

#+begin_src cpp
  class Base {
  public:
    auto foo(this auto&& self) -> int { return 77 + self.bar(); }
  };

  class Derived : public Base {
  public:
    auto bar() -> int { return 88; }
  };
#+end_src

This yields identical optimized code: ~foo~ is instantiated as
~foo<Derived>~, and the call to ~bar~ is resolved statically and
inlined.

* DONE [#B] About this Site
CLOSED: [2018-05-11 Mon 21:18]
:PROPERTIES:
:EXPORT_FILE_NAME: about-this-website
:END:
:LOGBOOK:
CLOCK: [2026-01-24 Sat 10:47]--[2026-01-24 Sat 10:47] =>  0:00
:END:

After nearly a decade, I've rebuilt my personal site from scratch---a
blog on software, self-hosting, and lessons learned along the way.  This
first post is backdated to when I originally launched a site on this
domain, back in 2018.

*What?*---A personal blog.  Posts aim to be concise, practical, and
rigorous.

*Why?*---To give back to the /libre/ software community.  To keep a
record of my learning journey.  To deepen my own understanding by
writing things down.[fn:23]

*Who?*---See the About page.

*Where?*---Self-hosted at my mother's house in northern Spain (ha!),
exposed to the Internet through a WireGuard tunnel to a VPS, and served
via a global CDN.

*When?*---Once or twice a month.

*How?*---Written in Emacs Org-mode, exported to Hugo, with a custom
theme.[fn:22]

If you spot an error or have suggestions, I welcome feedback---reach out
via the contact link on the About page.

* DONE [#B] David Álvarez Rosa
CLOSED: [2026-01-24 Sat 12:31]
:PROPERTIES:
:EXPORT_FILE_NAME: about
:EXPORT_HUGO_SECTION: .
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :subtitle Curriculum Vitae
:END:

#+begin_export html
<style>
  .signature { display: none; }
  sup { display: none; }
  .side img { max-width: 225px; box-shadow: none; margin-top: -26px;}
  @media (max-width: 860px) { .side img { margin-top: 0!important; } }
  main p:first-of-type::first-letter { float: revert; font-size: revert; font-family: revert; padding: revert; }
</style>
#+end_export

 [fn:8]Mathematician and engineer based in sunny Dublin, passionate
about low-latency, high-performance systems.

Currently working in algorithmic trading at Susquehanna.  Previously
designed and built systems at Amazon serving 10M+ monthly active
customers, developed semantic caching for LLMs at Sopra Steria, and
conducted quantitative cybersecurity risk analysis at Deloitte.

Holds a BSc in Mathematics, a BEng in Industrial Engineering, and an MSc
in Artificial Intelligence.

** Experience

/Software Engineer @ Susquehanna/
[fn::Jul 2024--Present\\
Dublin, Ireland]\\
High-frequency options trading.  Low-latency market data and trading
signals.  Mentor and interviewer.

/Software Engineer II @ Amazon/
[fn::Mar 2022--Aug 2024\\
Madrid, Spain]\\
Designed systems for 10M+ monthly active customers.  Contributed to 100+
internal repos.  Won org hackathon.  Promoted in 18 months (top 5%).
Mentored 3.  On-call.  Interviewer.

/Machine Learning Engineer @ Sopra Steria/
[fn::Apr 2024--Jul 2024\\
Remote]\\
Researched, designed, and built a semantic cache for LLMs.

/Risk Analyst @ Deloitte/
[fn::Sep 2021--Mar 2022\\
Madrid, Spain]\\
Quantitative analysis of technological and cybersecurity risks for
top-tier banking companies.

/Visiting Researcher @ Vector Institute/
[fn::Sep 2020--Jun 2021\\
Toronto, Canada]\\
Research thesis on multimodal learning (recomprehension.com).

/Machine Learning Engineer @ BCN eMotorsport/
[fn::Sep 2019--Feb 2020\\
Barcelona, Spain]\\
Perception at Driverless UPC.  I served as LiDAR lead and collaborated
on computer vision for a fully autonomous car.

** Education

/MSc in Artificial Intelligence/
[fn::GPA 9.00/10\\
Honors in 6 subjects]\\
Official study program focused on AI research and enabling PhD.

/MSc in Mathematics/\\
Math-lover part-time student. Dropout (joined Amazon).

/Research Thesis/
[fn::GPA 10/10 (A+)]\\
Research thesis on multimodal learning at University of Toronto.

/BSc in Mathematics/
[fn::GPA 8.12/10 (top 10%)\\
Honors in 9 subjects]\\
Rigorous and proof-oriented degree with a robust mathematical base.

/BEng in Industrial Engineering/
[fn::GPA 8.03/10 (top 2%)\\
Honors in 14 subjects]\\
Multidisciplinary and integrative vision of industrial engineering.

** Licenses & certifications

Certificate in Advanced English (C1) --- Cambridge University\\
Machine Learning --- Stanford University\\
Deep Learning --- deeplearning.ai\\
Blockchain & Financial Technology --- Hong Kong University\\
Nova Talent Member --- Nova

** Volunteering

/Mathematics Tutor/\\
Academic training for the Mathematical Olympiads.

/Volunteer @ Banco de Alimentos/\\
Food collection for people in need.

** Honors & awards

/Mathematical Olympiad/\\
Silver in local (Pamplona), honors in national (Barcelona).

/Physics Olympiad/\\
Gold in local (Pamplona), silver in national (Seville).

/Mobility Scholarship --- Cellex (CFIS)/
[fn::Canceled due to Covid-19]\\
Scholarship to carry out my research thesis at Toronto (€6k).

/Tuition and Housing Scholarship --- Cellex (CFIS)/\\
University tuition and housing (€19k).

/General Scholarship --- Government of Spain/\\
Full university tuition plus an annual stipend (€11k).

** Languages

English --- Proficient\\
Spanish --- Native\\
Catalan --- Intermediate

** Contact

You can reach me at david@alvarezrosa.com (preferred) or +34 647 13
39 30.

* DONE [#B] Welcome
CLOSED: [2025-12-02 Tue 19:58]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: .
:END:

 [fn:9]Hi! This is my personal site, where I share notes on software and
self-hosting.  You'll learn how things work under the hood, and how to
make them run fast, /very/ fast.

I'm a mathematician and engineer based in sunny Dublin, passionate about
low-latency, high-performance systems.  I'm a strong advocate of free
(as in /freedom/) software, and a devoted Emacs user.

Find me on [[https://github.com/david-alvarez-rosa/][GitHub]], [[https://gitlab.com/david-alvarez-rosa][GitLab]], and [[https://linkedin.com/in/david-alvarez-rosa][LinkedIn]].

* DONE [#B] Page Not Found
CLOSED: [2025-12-13 Sat 22:23]
:PROPERTIES:
:EXPORT_FILE_NAME: 404
:EXPORT_HUGO_SECTION: .
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :subtitle " "
:END:

#+begin_export html
<style> .signature { display: none; } main p:first-of-type::first-letter { float: revert; font-size: revert; font-family: revert; padding: revert; } </style>
#+end_export

That page couldn't be found (404 error).  Try the [[/][Home page]].

* DONE [#B] Subscription
CLOSED: [2025-12-13 Sat 22:06]
:PROPERTIES:
:EXPORT_FILE_NAME: subscription
:EXPORT_HUGO_SECTION: .
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :subtitle " "
:END:

#+begin_export html
<style> .signature { display: none; } main p:first-of-type::first-letter { float: revert; font-size: revert; font-family: revert; padding: revert; } </style>
#+end_export

Welcome! Your subscription is confirmed.  Thanks for signing up.

You can unsubscribe anytime by emailing david@alvarezrosa.com from the
same email address you subscribed with.

* Footnotes
[fn:24] For a Rust version, see Waifod's post [[https://waifod.dev/blog/polymorphism-type-erasure/][Polymorphism in C++ and
Rust: Type Erasure]].

[fn:25] ~release~ ensures prior writes are visible to threads that
~acquire~ the same variable.  Both sides use ~relaxed~ for their own
index since no other thread writes to it.

[fn:23] As Einstein put it, /"If you can't explain it simply, you don't
understand it well enough."/ Feynman followed the same principle,
testing his understanding by teaching from first principles.

[fn:22] Inspired by the work of Edward Tufte and Matthew Butterick.

[fn:21] Alongside ~-O3~, the benchmark was compiled with ~-march=native~
and ~-ffast-math~, though these flags shouldn't make a difference here.

[fn:20] Since you're employing polymorphism in the first place, most
callers will likely fall into the second group, and will need to be
templates themselves too so they can pass the type through.  That can
quickly spread templates across the codebase, making it harder to read
and structure, increasing compile times, and producing larger binaries
with slower startup.

[fn:19] The type erasure concept is an OO-style interface (a vtable).
It's unrelated to C++20 ~concept~ (compile-time predicates).

[fn:18] This implementation always heap-allocates.  Production
~std::any~ implementations often use small buffer optimization (SBO)
techniques to store small objects inline and avoid allocation.

[fn:17] It's useful to observe the number of cache misses with ~perf
stat -e cache-misses~; they are greatly reduced in this approach.

[fn:16] This advanced optimization was initially proposed by [[https://rigtorp.se][Erik
Rigtorp]].

[fn:15] Note that we are manually aligning ~alignas~ the atomics to
ensure they fall in different cache lines (commonly 64 bytes).  This
prevents false sharing, hence optimizes CPU cache usage.

[fn:14] Compiled with ~clang~ compiler with highest ~-O3~ optimization
level, and ~-march=native -ffast-math~.  Consumer and producer threads
are pinned to dedicated cores (Intel Core Ultra 5 135U).  See [[https://github.com/david-alvarez-rosa/CppPlayground/blob/main/DataStructures/ring_buffer.cpp][benchmark]].

[fn:13] If you tried to pass in a type that doesn't conform to the
'interface' (say, ~std::string~), the compiler would hit an error when
you try to compile the method call, complaining that ~std::string~
doesn't have an ~area~ method.

[fn:12] In some cases, you may not have control of the concrete types
(e.g. think STL types like ~std::string~), or it may not even be
possible for the concrete type to inherit (e.g. builtins like ~int~).

[fn:10] Remember that interfaces that are intended to be used through a
~Base&~ or ~Base*~ must have a virtual destructor, to ensure derived
classes are properly destructed [[https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#c127-a-class-with-a-virtual-function-should-have-a-virtual-or-protected-destructor][(C.127)]].

[fn:9] [[./static/images/home-illustration.png]] ‎

[fn:8] [[./static/images/portrait.png]] *That's me!* March 2022.

[fn:11] [[./static/images/ringbuffer.jpg]] *Ring buffer with 32 slots.* The
producer has filled 15 of them, indicated by blue.  The consumer is
behind the producer, reading data from the slots, freeing them as it
does so.  A free slot is indicated by orange.

[fn:1] By using ~std::array~ we are forcing clients to define the buffer
size as ~constexpr~.  It's also common to use instead a ~std::vector~ to
remove that restriction.  A further optimization is to constrain the
capacity to a power of two, allowing wrap-around via bit masking ~head &
(N - 1)~ instead of a branch.

[fn:2] Note how one item is left unused to indicate that the queue is
full.  When ~head_~ is one item ahead of ~tail_~, the queue is full.

[fn:3] Again note that ~head_ == tail_~ indicates that the queue is
empty.

[fn:4] Assembly generated with ~gcc~ at ~-O3~ on x86-64.  Similar
results were observed with ~clang~ on the same platform.

[fn:5] The compiler emits a direct call to ~Derived::foo~ (or inlines
it), because ~derived~ cannot have any other dynamic type.

[fn:6] The curiously recurring template pattern is an idiom where a
class X derives from a class template instantiated with X itself as a
template argument.  More generally, this is known as F-bound
polymorphism, a form of F-bounded quantification.

[fn:7] The trade-off is that each ~Base<Derived>~ instantiation is a
distinct, unrelated type, so there's no common runtime base to upcast
to.  Any shared functionality that operates across different derived
types must itself be templated.
