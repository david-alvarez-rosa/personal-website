#+startup: logdone
#+macro: marginnote @@hugo:{{< marginnote >}}$1{{< /marginnote >}}@@
#+macro: sidenote @@hugo:{{< sidenote >}}$1{{< /sidenote >}}@@
#+macro: marginfig @@hugo:{{< figure src="/ox-hugo/$1" width="margin" caption="$2" >}}@@
#+macro: newthought @@hugo:{{< newthought >}}$1{{< /newthought >}}@@

* DONE Welcome!
CLOSED: [2025-11-25 Tue 12:58]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: .
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2025-11-25 Tue 12:58]
- State "TODO"       from "DONE"       [2025-11-25 Tue 12:58]
- State "DONE"       from "TODO"       [2025-11-25 Tue 12:57]
- State "TODO"       from              [2025-11-25 Tue 12:57]
:END:

Hello! This is David, a software engineer.
Software engineer with experience in low-latency systems, electronic
trading, and HFT infrastructure. Currently at Susquehanna. Strong
advocate of free (as in freedom) software and devoted Emacs user.

Modern CPUs can execute billions of instructions per second, yet your
code can still be bottlenecked by something as mundane as how an array
is laid out in memory.

* DONE Rethinking Performance: A Developer’s Guide to CPU Cache Lines :software:
CLOSED: [2024-10-12 Tue 09:31]
:PROPERTIES:
:EXPORT_FILE_NAME: my-first-post
:END:
Modern CPUs can execute billions of instructions per second, yet your code can still be bottlenecked by something as mundane as how an array is laid out in memory. The reason is simple: CPUs are /much/ faster than main memory, so almost every performance-critical program ends up being constrained not by computation, but by data movement. At the center of this tension is a deceptively small unit: the CPU cache line.

Cache lines are how data travels between your code and the CPU’s caches. They determine what gets fetched, what gets evicted, and how different cores interfere with each other’s data. Whether you’re chasing nanoseconds in a trading system, optimizing a game engine, or just trying to make a tight loop scale across cores, understanding cache lines is often the key to unlocking the “mystery” of why some code is fast and other code isn’t.

* DONE From Threads to Tasks: Rethinking Concurrency in Modern C++
CLOSED: [2025-11-25 Tue 09:31]
:PROPERTIES:
:EXPORT_FILE_NAME: my-first-postiuiui
:END:
For years, writing concurrent C++ meant thinking in terms of threads,
mutexes, and condition variables. You’d spin up a few =std::thread= s, protect shared data with locks, and hope you got the ordering and lifetime rules right. As codebases and core counts grew, this model started to creak: too many threads, too much blocking, and too much complexity to reason about correctness and performance.

Modern C++ is steadily moving away from “managing threads” toward “expressing tasks.” Instead of deciding which thread runs what, you describe /what/ needs to happen, /when/ it can happen, and let an executor or scheduler worry about mapping that work onto hardware. This shift—from threads to tasks—enables better scalability, clearer structure, and often fewer bugs.

* DONE Beyond Big-O: Why Constant Factors Still Matter             :software:
CLOSED: [2025-12-01 Mon 18:52]
:PROPERTIES:
:EXPORT_FILE_NAME: my-first-postsd
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2025-12-01 Mon 18:52]
- State "TODO"       from "DONE"       [2025-12-01 Mon 18:52]
:END:
When we talk about algorithmic efficiency, the conversation almost
always revolves around Big-O notation. We proudly say our algorithm is
O(n log n) instead of O(n²) and call it a day. But in real-world
systems, the story doesn’t end there. Two algorithms with the /same/
asymptotic complexity can differ by an order of magnitude in
performance—and those “pesky constant factors” we hand-wave away in
theory suddenly matter a lot.

#+begin_src cpp
  auto main() -> int {
    return 0;
  }
#+end_src

In production code, constants show up everywhere: cache misses, branch mispredictions, allocations, lock contention, system calls, network round trips, and more. Each one adds a fixed cost that doesn’t change your Big-O, but absolutely changes your latency and throughput. An O(n) solution that touches memory in the wrong pattern can be slower than an O(n log n) one that plays nicely with hardware. Likewise, a “clean” but allocation-heavy design might fall apart under load compared to a more careful implementation with the same complexity on paper.

* DONE Exploring CPU Cache Lines                            :tag1:@category1:
CLOSED: [2025-11-25 Tue 09:31]
:PROPERTIES:
:EXPORT_FILE_NAME: second-post
:END:
Modern CPUs are astonishingly fast on paper, yet in real-world code they
often spend a surprising amount of time waiting for data. The gap
between how fast a processor can execute instructions and how fast it
can fetch data from main memory is enormous. To bridge that gap,
hardware designers introduced a hierarchy of caches, with the CPU cache
line sitting at the center of how data actually moves.

* DONE Implementing a Single Producer Single Consumer Queue :tag1:@category1:
CLOSED: [2025-11-25 Tue 09:31]
:PROPERTIES:
:EXPORT_FILE_NAME: third-post
:END:
Lock-free queues are a staple of high-performance systems, but their
implementations often look like black magic. In this post we’ll build a
simple *single-producer single-consumer (SPSC)* queue from scratch in
C++, explain the core ideas, and discuss correctness and performance.

The constraints:

- Only *one producer thread* calls =push=.
- Only *one consumer thread* calls =pop=.
- No locks (=std::mutex=) in the data path.
- Bounded capacity (fixed-size ring buffer).

* DONE Devirtualization and Static Polymorphism
CLOSED: [2025-12-02 Tue 15:57]
:PROPERTIES:
:EXPORT_FILE_NAME: my-draft
:END:
:LOGBOOK:
- State "DONE"       from "WAIT"       [2025-12-02 Tue 15:57]
- State "WAIT"       from "TODO"       [2025-12-02 Tue 15:57]
- State "TODO"       from "DONE"       [2025-12-02 Tue 15:57]
- State "DONE"       from "TODO"       [2025-12-01 Mon 18:52]
- State "TODO"       from "DONE"       [2025-12-01 Mon 18:52]
- State "DONE"       from "TODO"       [2025-12-01 Mon 18:49]
- State "TODO"       from              [2025-12-01 Mon 18:49]
:END:

Virtual dispatch is the basis of C++ runtime polymorphism, but it comes
with hidden overhead: pointer indirection, larger object layouts, and
lost inlining opportunities.  /Devirtualization/ lets the compiler
recover some of this by turning virtual calls into direct calls when it
can prove the dynamic type.

Unfortunately, that's rarely the case.

For latency-sensitive paths, it's beneficial to manually replace dynamic
dispatch with /static polymorphism/, so calls are resolved at compile
time and the abstraction has effectively zero runtime cost.

** Virtual dispatch

Runtime polymorphism is achieved when a base interface exposes virtual
methods that derived classes can override.  Calls made through ~Base&~
are dispatched to the appropriate override dynamically.  Under the hood,
a virtual table (~vtable~) of function pointers and a ~vptr~ pointing at
that table is used. A virtual call loads the ~vptr~, looks up the right
entry in the ~vtable~, and performs an indirect call.

#+begin_src plantuml :file diagram.png
  @startuml

  hide empty members

  interface Base {
    + virtual auto foo()
  }

  class Derived {
    + auto foo() override
  }

  Base <|-- Derived

  object BaseVT <<vtable>> {
    + &Base::foo
  }

  object DerivedVT <<vtable>> {
    + &Derived::foo
  }

  object BaseObject {
    - vptr : *vtable
    - ...members...
  }

  object DerivedObject {
    - vptr : *vtable
    - ...members...
  }

  object DerivedObject2 {
    - vptr : *vtable
    - ...members...
  }

  BaseObject --> BaseVT : vptr
  DerivedObject --> DerivedVT : vptr
  DerivedObject2 --> DerivedVT : vptr

  @enduml
#+end_src

#+caption: asdf.  Diagram created by the author.
#+results:
[[file:diagram.png]]

The ~vptr~ increases object size and can hurt cache locality.  The
~vtable~ makes indirect calls harder to predict, so they can result in
branch mispredictions and the reduced compiled-time knowledge prevents
inlining and other optimizations.

*** Inspecting assembly

To understand why virtual calls can hurt performance, it helps to look at what
the compiler actually emits for a simple example.
#+begin_src cpp
  class Base {
  public:
    auto foo() -> int;
  };

  auto bar(Base* base) -> int {
    return base->foo() + 77;
  }
#+end_src

For a plain, non-virtual member function, ~bar~ has a direct
call{{{sidenote(Assembly generated on x86-64 system with `gcc` and `-O3`
optimization level.  Similar results were found using `clang` in that
same platform.)}}} as the target is known at compile
time, so the compiler can inline it, propagate constants, and optimize
across the call boundary.
#+begin_src asm
  bar(Base*):
          sub     rsp, 8
          call    Base::foo()  // Direct call
          add     rsp, 8
          add     eax, 77
          ret
#+end_src

Now, marking ~foo~ as virtual changes ~bar~ from a direct call to an indirect,
vtable-based call.
#+begin_src asm
  bar(Base*):
          sub     rsp, 8
          mov     rax, QWORD PTR [rdi]
          call    [QWORD PTR [rax]]  // Virtual call
          add     rsp, 8
          add     eax, 77
          ret
#+end_src

Those extra loads are potential cache misses; the indirect branch is
harder for the CPU to predict.  More importantly, because the target is
not known statically, the compiler generally cannot inline ~foo~ or
propagate constants from its caller into its body.

** Compiler devirtualization

/Devirtualization/ is the process by which the compiler recovers some of
that lost information. When it can prove at compile time which override
will be called, it can skip the ~vtable~ lookup and emit a direct call
instead.

Once the cost of virtual calls is clear, the next question is when the
compiler can get rid of them.  Modern C++ compilers will devirtualize a
call whenever they can prove the dynamic type of the object.

That's easy when the type is obviously fixed:

#+begin_src cpp
  struct Base {
    virtual auto foo() -> int = 0;
  };

  struct Derived : Base {
    auto foo() -> int override { return 77; }
  };

  auto bar() -> int{
    Derived derived;
    return derived.foo();  // compiler knows this is Derived::foo
  }
#+end_src

Here, the compiler{{{sidenote(It can even devirtualize through a base
pointer\, as long as it can track the allocation and see there's only
one possible concrete type.  The problem is that with traditional
separate compilation\, objects are often created in one translation unit
and used in another\, so that global view is missing.)}}} can emit a
direct call to ~Derived::foo~ (or inline it) because ~derived~ can't be
anything else.

With compilers, a translation unit consists of a single ~.cpp~ file that
is compiled, optimized in isolation, and only then turned into object
code.  The linker just combines the resulting objects, so cross-TU
optimizations are limited.  This is where ~-fwhole-program~ and ~-flto~
come in.

- ~-fwhole-program~ :: tells the compiler "this translation unit is the
  whole program".  If no class derives from ~Base~ in this TU, the
  compiler is free to assume nothing ever does, and it can devirtualize
  calls on ~Base~.
- ~-flto~ :: (link‑time optimization) keeps intermediate representation
  in the object files and runs optimization at link time over all of
  them together.  Multiple source files are effectively treated as one
  big unit, which enables more devirtualization, inlining, and constant
  propagation across file boundaries.

On the language side, ~final~ is a lightweight way to give the compiler
similar certainty for individual methods:

#+begin_src cpp
  class Base {
  public:
    virtual auto foo() -> int;
    virtual auto bar() -> int;
  };

  class Derived : public Base {
  public:
    auto foo() -> int override;
    auto bar() -> int final;
  };

  auto test(Derived* derived) -> int {
    return derived->foo() + derived->bar();
  }
#+end_src

Here, ~foo()~ can still be overridden by a class derived from
~Derived~, so ~derived->foo()~ is compiled as a virtual call. ~bar()~ is
marked ~final~, so the compiler knows no further overrides exist and can
emit a direct call to ~Derived::bar~ (and inline it) even though it’s
declared ~virtual~ in the base.

#+begin_src asm
  test(Derived*):
          push    rbx
          sub     rsp, 16
          mov     rax, QWORD PTR [rdi]
          mov     QWORD PTR [rsp+8], rdi
          call    [QWORD PTR [rax]]       // Virtual call
          mov     rdi, QWORD PTR [rsp+8]
          mov     ebx, eax
          call    Derived::bar()          // Direct call
          add     rsp, 16
          add     eax, ebx
          pop     rbx
          ret
#+end_src


** Static polymorphism

When the compiler can’t devirtualize on its own, one option is to stop using dynamic dispatch on the hot path and switch to static polymorphism. The classic way to do that in C++ is the Curiously Recurring Template Pattern (CRTP).

With CRTP, the base class is templated on the derived class, and instead of calling virtual methods, it calls into the derived type via ~static_cast~:

#+begin_src cpp
  template <typename Derived>
  class Base {
  public:
    auto foo() -> int {
      return 77 + static_cast<Derived*>(this)->bar();
    }
  };

  class Derived : public Base<Derived> {
  public:
    auto bar() -> int {
      return 88;
    }
  };

  auto test() -> int {
    Derived derived;
    return derived.foo();
  }
#+end_src

With ~-O3~, ~test~ is compiled down to:

#+begin_src asm
  test():
          mov     eax, 165  // 77 + 88
          ret
#+end_src

The compiler inlines ~foo~ and ~bar~ and constant-folds the result;
there are no vtables and no virtual calls. The trade‑off is that each
~Base<Derived>~ instantiation is its own unrelated type, so there’s no
single runtime base you can upcast to. Shared functionality that works
across different derived types has to be templated as well.

*** Deducing this

C++23’s /deducing this/ feature keeps the same static‑dispatch idea but
makes it easier to write and understand. Instead of templating the whole
class (and spelling ~Base<Derived>~ everywhere), you template just the
member function that needs to see the derived type, and let the compiler
deduce ~self~ from ~*this~:

#+begin_src cpp
  class Base {
   public:
    auto foo(this auto&& self) -> int { return 77 + self.bar(); }
  };

  class Derived : public Base {
   public:
    auto bar() -> int { return 88; }
  };

  auto test() -> int {
    Derived derived;
    return derived.foo();
  }
#+end_src

This generates essentially the same optimized code as the CRTP version:
~foo~ is instantiated as if it were a ~foo<Derived>~ and the call to
~bar~ is statically resolved and inlined. The key difference is
syntactic and structural:

- you still get static polymorphism (no vtables, fully optimizable calls),
- but you also keep a real ~Base~ type that all derived classes share.

It’s still not dynamic dispatch: a call routed through a ~Base*~ only
sees what ~Base~ defines unless you also use ~virtual~. But for
performance‑critical paths where the concrete type is known, CRTP and
C++23’s deducing‑this style give the compiler exactly what it needs to
generate non‑virtual, highly optimized code.


* DONE Type Erasure
CLOSED: [2025-12-02 Tue 16:05]
:PROPERTIES:
:EXPORT_FILE_NAME: my-draft-other
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2025-12-02 Tue 16:05]
- State "TODO"       from              [2025-12-02 Tue 16:05]
:END:

Hello.

* DONE David Álvarez Rosa
CLOSED: [2025-11-25 Tue 09:48]
:PROPERTIES:
:EXPORT_FILE_NAME: about
:EXPORT_HUGO_SECTION: .
:EXPORT_HUGO_LAYOUT: about
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2025-11-25 Tue 09:48]
- State "TODO"       from              [2025-11-25 Tue 09:48]
:END:

{{{marginfig(portrait.png, **That's me!** March 2022.)}}}
Software engineer with experience in low-latency systems, electronic
trading, and HFT infrastructure. Currently at Susquehanna.  Strong
advocate of free (as in /freedom/) software and devoted Emacs user.

Previously designed and implemented embedded systems at Amazon impacting
10M+ monthly active customers, developed semantic caching for LLMs at
Sopra Steria, and conducted quantitative analysis of cybersecurity risks
at Deloitte.

Holds a BSc in Mathematics, a BEng in Industrial Engineering, and a MSc
in Artificial Intelligence.

** Experience
*Software Engineer* --- Susquehanna
{{{marginnote(Jul 2024--Present<br/>Dublin\, Ireland)}}}\\
High-frequency options trading.  Low-latency market data and
trading signals.  Mentor and interviewer.

*Software Engineer II* --- Amazon
{{{marginnote(Mar 2022--Aug 2024<br/>Madrid\, Spain)}}}\\
Contributed to 100+ internal repos.  Won org hackathon.  Designed
systems for 10M+ monthly active customers.  Promoted in 18 months (top
5%).  Mentored 3.  On-call.  Interviewer.

*Machine Learning Engineer* --- Sopra Steria
{{{marginnote(Apr 2024--Jul 2024<br/>Remote)}}}\\
Researched, designed, and built a semantic cache for LLMs.

*Risk Analyst* --- Deloitte
{{{marginnote(Sep 2021--Mar 2022<br/>Madrid\, Spain)}}}\\
Quantitative analysis of technological and cybersecurity risks for
top-tier banking companies.

*Visiting Researcher* --- Vector Institute
{{{marginnote(Sep 2020--Jun 2021<br/>Toronto\, Canada)}}}\\
Research thesis on multimodal learning (recomprehension.com).

*Machine Learning Engineer* --- BCN eMotorsport
{{{marginnote(Sep 2019--Feb 2020<br/>Barcelona\, Spain)}}}\\
Perception at Driverless UPC.  I served as LiDAR lead and collaborated
on computer vision for a fully autonomous car.

** Education
*MSc in Artificial Intelligence* --- UNIR
{{{marginnote(GPA 9.00/10)}}}\\
Official study program focused on AI research and enabling PhD.

*MSc in Mathematics* --- UNED\\
Math-lover part-time student. Dropout (joined Amazon).

*Research Thesis* --- UofT
{{{marginnote(GPA 10/10 (A+))}}}\\
Research thesis on multimodal learning (recomprehension.com).

*BSc in Mathematics* --- UPC
{{{marginnote(GPA 8.12/10 (top 10%)<br/>Honors in 9 subjects)}}}\\
Rigorous and proof-oriented degree with a robust mathematical base.

*BEng in Industrial Engineering* --- UPC
{{{marginnote(GPA 8.03/10 (top 2%)<br/>Honors in 14 subjects)}}}\\
Multidisciplinary and integrative vision of industrial engineering.

** Licenses & certifications
*Certificate in Advanced English (C1)* --- Cambridge University\\
*Machine Learning* --- Stanford University\\
*Deep Learning* --- deeplearning.ai\\
*Blockchain & Financial Technology* --- Hong Kong University\\
*Nova Talent Member* --- Nova

** Volunteering
*Mathematics Tutor*\\
Academic training for the Mathematical Olympiads.

*Volunteer* --- Banco de Alimentos\\
Food collection for people in need.

** Honors & awards
*Mathematical Olympiad*\\
Silver in local (Pamplona), honors in national (Barcelona).

*Physics Olympiad*\\
Gold in local (Pamplona), silver in national (Seville).

*Mobility Scholarship* --- Cellex (CFIS)
{{{marginnote(Canceled due to Covid-19)}}}\\
Scholarship to carry out my research thesis at Toronto (€6k).

*Tuition and Housing Scholarship*  --- Cellex (CFIS)\\
University tuition and housing (€19k).

*General Scholarship* --- Government of Spain\\
Full university tuition plus an annual stipend (€11k).

** Languages
*English* --- Proficient\\
*Spanish* --- Native\\
*Catalan* --- Intermediate
