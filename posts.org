#+startup: logdone
#+macro: marginnote @@hugo:{{< marginnote >}}$1{{< /marginnote >}}@@
#+macro: sidenote @@hugo:{{< sidenote >}}$1{{< /sidenote >}}@@
#+macro: marginfig @@hugo:{{< figure src="/ox-hugo/$1" width="margin" caption="$2" >}}@@
#+macro: newthought @@hugo:{{< newthought >}}$1{{< /newthought >}}@@

* TODO Exploring CPU Caches
:PROPERTIES:
:EXPORT_FILE_NAME: exploring-cpu-caches
:END:
:LOGBOOK:
- State "TODO"       from "DONE"       [2025-12-02 Tue 19:55]
:END:

Pending.

* TODO Single Producer Single Consumer Queue
:PROPERTIES:
:EXPORT_FILE_NAME: single-producer-single-consumer-queue
:END:
:LOGBOOK:
- State "TODO"       from "DONE"       [2025-12-02 Tue 19:54]
:END:

Pending.

* TODO Type Erasure
:PROPERTIES:
:EXPORT_FILE_NAME: type-erasure
:END:
:LOGBOOK:
- State "TODO"       from "DONE"       [2025-12-03 Wed 14:40]
- State "DONE"       from "TODO"       [2025-12-02 Tue 16:05]
- State "TODO"       from              [2025-12-02 Tue 16:05]
:END:

Pending.

On latency-sensitive paths, it's beneficial to manually replace dynamic
dispatch with /static polymorphism/, so calls are resolved at compile
time and the abstraction has effectively zero runtime cost.

* DONE Devirtualization and Static Polymorphism
CLOSED: [2025-12-02 Tue 15:57]
:PROPERTIES:
:EXPORT_FILE_NAME: devirtualization-and-static-polymorphism
:END:
:LOGBOOK:
- State "DONE"       from "WAIT"       [2025-12-02 Tue 15:57]
- State "WAIT"       from "TODO"       [2025-12-02 Tue 15:57]
- State "TODO"       from "DONE"       [2025-12-02 Tue 15:57]
- State "DONE"       from "TODO"       [2025-12-01 Mon 18:52]
- State "TODO"       from "DONE"       [2025-12-01 Mon 18:52]
- State "DONE"       from "TODO"       [2025-12-01 Mon 18:49]
- State "TODO"       from              [2025-12-01 Mon 18:49]
:END:

Virtual dispatch is the basis of C++ runtime polymorphism, but it comes
with non-trivial overhead: pointer indirection, larger object layouts,
and fewer inlining opportunities.  /Devirtualization/ lets the compiler
recover some of this by turning virtual calls into direct calls when it
can infer the dynamic type.

Unfortunately, that is often not possible.

On latency-sensitive paths, it's beneficial to manually replace dynamic
dispatch with /static polymorphism/, so calls are resolved at compile
time and the abstraction has effectively zero runtime cost.

** Virtual dispatch

Runtime polymorphism arises when a base interface exposes virtual
methods that derived classes override.  Calls made through a ~Base&~ (or
~Base*~) are then dispatched to the appropriate override at runtime.
Under the hood, it works roughly like this:

- Each polymorphic class has a virtual table (~vtable~) that holds the
  function pointers for its virtual methods.

- Each object of such a class carries a hidden pointer (~vptr~) to the
  corresponding ~vtable~.

On a virtual call, the compiler emits code that loads the ~vptr~,
selects the right slot in the ~vtable~, and performs an indirect call
through that function pointer.

#+begin_src plantuml :file diagram.png
  @startuml

  hide empty members

  interface Base {
    + virtual auto foo()
  }

  class Derived {
    + auto foo() override
  }

  Base <|-- Derived

  class ZZZ
  hide ZZZ

  object BaseVT <<vtable>> {
    + &Base::foo
  }

  object DerivedVT <<vtable>> {
    + &Derived::foo
  }

  object BaseObject {
    - vptr : *vtable
    - ...members...
  }

  object DerivedObject {
    - vptr : *vtable
    - ...members...
  }

  object DerivedObject2 {
    - vptr : *vtable
    - ...members...
  }

  BaseObject --> BaseVT : vptr
  DerivedObject --> DerivedVT : vptr
  DerivedObject2 --> DerivedVT : vptr

  @enduml
#+end_src

#+caption: *Virtual dispatch diagram.*  The method ~foo~ is declared virtual in ~Base~ and overridden in ~Derived~.  Both classes get a ~vtable~, and each object gets a ~vptr~ pointing to the corresponding ~vtable~.  Diagram created by the author.
#+results:
[[file:diagram.png]]

The additional ~vptr~ increases object size, which can hurt cache
locality.  The ~vtable~ makes the call target harder to predict, raising
the chance of branch mispredictions, and the lack of compile-time
knowledge prevents inlining and other optimizations.

*** Inspecting assembly

To see why virtual calls can be costly, it's useful to inspect the code
the compiler actually emits for a minimal example.

#+begin_src cpp
  class Base {
  public:
    auto foo() -> int;
  };

  auto bar(Base* base) -> int {
    return base->foo() + 77;
  }
#+end_src

For a regular, non-virtual member function like in the example, the free
function ~bar~ issues a direct call{{{sidenote(Assembly generated on an
x86-64 system with `gcc` at `-O3`.  Similar results were observed with
`clang` on the same platform.)}}} to ~foo~.  Because the target is known
at compile time, the compiler can inline it, propagate constants, and
optimize across the call boundary.

#+begin_src asm
  bar(Base*):
          sub     rsp, 8
          call    Base::foo()  // Direct call
          add     rsp, 8
          add     eax, 77
          ret
#+end_src

However, declaring ~foo~ as virtual changes ~bar~'s assembly from a
direct call into an indirect, vtable-based call.

#+begin_src asm
  bar(Base*):
          sub     rsp, 8
          mov     rax, QWORD PTR [rdi]  // vptr (pointer to vtable)
          call    [QWORD PTR [rax]]  // Virtual call
          add     rsp, 8
          add     eax, 77
          ret
#+end_src

Those extra loads are potential cache misses, and the indirect branch is
harder for the CPU to predict.  More importantly, because the call
target isn't known statically, the compiler generally can't inline ~foo~
or propagate constants from the caller into its body.

** Devirtualization

The process by which the compiler statically determines which override a
virtual call will hit is called /devirtualization/.  When it can prove
at compile time which implementation will be used, it can skip the
~vtable~ lookup and emit a direct call instead.

For example, devirtualization is straightforward when the dynamic type
is clearly fixed:

#+begin_src cpp
  struct Base {
    virtual auto foo() -> int = 0;
  };

  struct Derived : Base {
    auto foo() -> int override { return 77; }
  };

  auto bar() -> int{
    Derived derived;
    return derived.foo();  // compiler knows this is Derived::foo
  }
#+end_src

Here, the compiler{{{sidenote(It can even devirtualize through a base
pointer\, as long as it can track the allocation and prove there is only
one possible concrete type.  The problem is that with traditional
separate compilation\, objects are often created in one translation unit
and used in another\, so that global view is missing.)}}} can emit a
direct call to ~Derived::foo~ (or inline it), because ~derived~ cannot
have any other dynamic type.

In C++, a translation unit (TU) is a single preprocessed ~.cpp~ file
that gets compiled and optimized in isolation, then emitted as object
code.  The linker simply stitches those objects together, so cross-TU
optimizations are inherently limited.  That's where compiler flags are
useful.

- ~-fwhole-program~ :: tells the compiler "this translation unit is the
  entire program." If no class derives from ~Base~ in this TU, the
  compiler is free to assume nothing ever does, and can devirtualize
  calls on ~Base~.

- ~-flto~ :: (link-time optimization) keeps an intermediate
  representation in the object files and performs optimization at link
  time across all of them together.  Multiple source files are
  effectively treated as a single large TU, which enables more
  devirtualization, inlining, and constant propagation across file
  boundaries.

On the language side, ~final~ is a lightweight way to give the compiler
comparable guarantees for specific methods.

#+begin_src cpp
  class Base {
  public:
    virtual auto foo() -> int;
    virtual auto bar() -> int;
  };

  class Derived : public Base {
  public:
    auto foo() -> int override;
    auto bar() -> int final;
  };

  auto test(Derived* derived) -> int {
    return derived->foo() + derived->bar();
  }
#+end_src

Here, ~foo()~ can still be overridden by a subclass of ~Derived~, so
~derived->foo()~ remains a virtual call.  ~bar()~, however, is marked
~final~, so the compiler knows there can be no further overrides and can
emit a direct call to ~Derived::bar~ (and inline it) even though it's
declared ~virtual~ in the base.

#+begin_src asm
  test(Derived*):
          push    rbx
          sub     rsp, 16
          mov     rax, QWORD PTR [rdi]
          mov     QWORD PTR [rsp+8], rdi
          call    [QWORD PTR [rax]]       // Virtual call
          mov     rdi, QWORD PTR [rsp+8]
          mov     ebx, eax
          call    Derived::bar()          // Direct call
          add     rsp, 16
          add     eax, ebx
          pop     rbx
          ret
#+end_src

** Static polymorphism

When the compiler can't devirtualize on its own, one option is to drop
dynamic dispatch and use static polymorphism instead.  The canonical C++
tool for this is the Curiously Recurring Template Pattern{{{sidenote(The
curiously recurring template pattern is an idiom where a class X derives
from a class template instantiated with X itself as a template argument.
More generally, this is known as F-bound polymorphism, a form of
F-bounded quantification.)}}} (CRTP).

With CRTP, the base class is templated on the derived class, and instead
of invoking virtual methods, it calls into the derived type via a
~static_cast~.

#+begin_src cpp
  template <typename Derived>
  class Base {
  public:
    auto foo() -> int {
      return 77 + static_cast<Derived*>(this)->bar();
    }
  };

  class Derived : public Base<Derived> {
  public:
    auto bar() -> int {
      return 88;
    }
  };

  auto test() -> int {
    Derived derived;
    return derived.foo();
  }
#+end_src

With ~-O3~ enabled, ~test~ compiles down to:

#+begin_src asm
  test():
          mov     eax, 165  // 77 + 88
          ret
#+end_src

The compiler inlines ~foo~ and ~bar~ and constant-folds the result;
there are no vtables and no virtual calls.  The trade-off is that each
~Base<Derived>~ instantiation is a distinct, unrelated type, so there's
no common runtime base to upcast to.  Any shared
functionality{{{sidenote(Also\, no polymorphic containers
(`std::vector<Base*>`) unless you wrap things.)}}} that operates across
different derived types must itself be templated.

*** Deducing this

C++23's /deducing this/ keeps the same static-dispatch model but makes
it easier to write and reason about.  Instead of templating the entire
class (and writing ~Base<Derived>~ everywhere), you template only the
member function that needs access to the derived type, and let the
compiler deduce ~self~ from ~*this~:

#+begin_src cpp
  class Base {
  public:
    auto foo(this auto&& self) -> int { return 77 + self.bar(); }
  };

  class Derived : public Base {
  public:
    auto bar() -> int { return 88; }
  };

  auto test() -> int {
    Derived derived;
    return derived.foo();
  }
#+end_src

This yields essentially the same optimized code as the CRTP version:
~foo~ is instantiated as if it were ~foo<Derived>~, and the call to
~bar~ is resolved statically and inlined.  The key differences are
syntactic and structural:

- you still get static polymorphism (no vtables, fully optimizable
  calls),

- but you also retain a single ~Base~ type that all derived classes
  share.

It's still not dynamic dispatch: a call through a ~Base*~ only sees what
~Base~ exposes, unless you also use ~virtual~.  But for
performance-critical paths where the concrete type is known, both CRTP
and C++23's deducing-this approach give the compiler exactly what it
needs to emit non-virtual, highly optimized code.

* DONE David Álvarez Rosa
CLOSED: [2025-11-25 Tue 09:48]
:PROPERTIES:
:EXPORT_FILE_NAME: about
:EXPORT_HUGO_SECTION: .
:EXPORT_HUGO_LAYOUT: about
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2025-11-25 Tue 09:48]
- State "TODO"       from              [2025-11-25 Tue 09:48]
:END:

{{{marginfig(portrait.png, **That's me!** March 2022.)}}}
Software engineer with experience in low-latency systems, electronic
trading, and HFT infrastructure. Currently at Susquehanna.  Strong
advocate of free (as in /freedom/) software and devoted Emacs user.

Previously designed and implemented embedded systems at Amazon impacting
10M+ monthly active customers, developed semantic caching for LLMs at
Sopra Steria, and conducted quantitative analysis of cybersecurity risks
at Deloitte.

Holds a BSc in Mathematics, a BEng in Industrial Engineering, and a MSc
in Artificial Intelligence.

** Experience
*Software Engineer* --- Susquehanna
{{{marginnote(Jul 2024--Present<br/>Dublin\, Ireland)}}}\\
High-frequency options trading.  Low-latency market data and
trading signals.  Mentor and interviewer.

*Software Engineer II* --- Amazon
{{{marginnote(Mar 2022--Aug 2024<br/>Madrid\, Spain)}}}\\
Contributed to 100+ internal repos.  Won org hackathon.  Designed
systems for 10M+ monthly active customers.  Promoted in 18 months (top
5%).  Mentored 3.  On-call.  Interviewer.

*Machine Learning Engineer* --- Sopra Steria
{{{marginnote(Apr 2024--Jul 2024<br/>Remote)}}}\\
Researched, designed, and built a semantic cache for LLMs.

*Risk Analyst* --- Deloitte
{{{marginnote(Sep 2021--Mar 2022<br/>Madrid\, Spain)}}}\\
Quantitative analysis of technological and cybersecurity risks for
top-tier banking companies.

*Visiting Researcher* --- Vector Institute
{{{marginnote(Sep 2020--Jun 2021<br/>Toronto\, Canada)}}}\\
Research thesis on multimodal learning (recomprehension.com).

*Machine Learning Engineer* --- BCN eMotorsport
{{{marginnote(Sep 2019--Feb 2020<br/>Barcelona\, Spain)}}}\\
Perception at Driverless UPC.  I served as LiDAR lead and collaborated
on computer vision for a fully autonomous car.

** Education
*MSc in Artificial Intelligence* --- UNIR
{{{marginnote(GPA 9.00/10)}}}\\
Official study program focused on AI research and enabling PhD.

*MSc in Mathematics* --- UNED\\
Math-lover part-time student. Dropout (joined Amazon).

*Research Thesis* --- UofT
{{{marginnote(GPA 10/10 (A+))}}}\\
Research thesis on multimodal learning (recomprehension.com).

*BSc in Mathematics* --- UPC
{{{marginnote(GPA 8.12/10 (top 10%)<br/>Honors in 9 subjects)}}}\\
Rigorous and proof-oriented degree with a robust mathematical base.

*BEng in Industrial Engineering* --- UPC
{{{marginnote(GPA 8.03/10 (top 2%)<br/>Honors in 14 subjects)}}}\\
Multidisciplinary and integrative vision of industrial engineering.

** Licenses & certifications
*Certificate in Advanced English (C1)* --- Cambridge University\\
*Machine Learning* --- Stanford University\\
*Deep Learning* --- deeplearning.ai\\
*Blockchain & Financial Technology* --- Hong Kong University\\
*Nova Talent Member* --- Nova

** Volunteering
*Mathematics Tutor*\\
Academic training for the Mathematical Olympiads.

*Volunteer* --- Banco de Alimentos\\
Food collection for people in need.

** Honors & awards
*Mathematical Olympiad*\\
Silver in local (Pamplona), honors in national (Barcelona).

*Physics Olympiad*\\
Gold in local (Pamplona), silver in national (Seville).

*Mobility Scholarship* --- Cellex (CFIS)
{{{marginnote(Canceled due to Covid-19)}}}\\
Scholarship to carry out my research thesis at Toronto (€6k).

*Tuition and Housing Scholarship*  --- Cellex (CFIS)\\
University tuition and housing (€19k).

*General Scholarship* --- Government of Spain\\
Full university tuition plus an annual stipend (€11k).

** Languages
*English* --- Proficient\\
*Spanish* --- Native\\
*Catalan* --- Intermediate

* DONE Welcome!
CLOSED: [2025-12-02 Tue 19:58]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: .
:END:
:LOGBOOK:
- State "DONE"       from              [2025-12-02 Tue 19:58]
:END:

Hello! This is David, a mathematician and software engineer interested
in low-latency systems, electronic trading, and high-performance
computing.  I'm a strong advocate of free (as in /freedom/) software and
devoted Emacs user.

On latency-sensitive paths, it's beneficial to manually replace dynamic
dispatch with /static polymorphism/, so calls are resolved at compile
time and the abstraction has effectively zero runtime cost.
